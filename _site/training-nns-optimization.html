<!DOCTYPE html>
<html lang="en">
  <head>
    <script src="https://kit.fontawesome.com/123ecac47c.js" crossorigin="anonymous"></script><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="shortcut icon" type="image/png" href="/assets/portfolio.png">
<title>Training Neural Networks: Optimization</title>
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Training Neural Networks: Optimization | Weikai’s blog.</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Training Neural Networks: Optimization" />
<meta name="author" content="Weikai Mao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Optimization" />
<meta property="og:description" content="Optimization" />
<link rel="canonical" href="http://localhost:4000/training-nns-optimization.html" />
<meta property="og:url" content="http://localhost:4000/training-nns-optimization.html" />
<meta property="og:site_name" content="Weikai’s blog." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-08T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Training Neural Networks: Optimization" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Weikai Mao"},"dateModified":"2020-07-08T00:00:00+08:00","datePublished":"2020-07-08T00:00:00+08:00","description":"Optimization","headline":"Training Neural Networks: Optimization","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/training-nns-optimization.html"},"url":"http://localhost:4000/training-nns-optimization.html"}</script>
<!-- End Jekyll SEO tag -->


<meta name="google-site-verification" content="wXp8C1QlYKCpKxfXyFfQXEv9l5fJvcOi53ofYmOcaSA" />
<meta name="msvalidate.01" content="97F0BB32D312B808156DE357EA8474D3" />
<meta name="yandex-verification" content="690106a82d8966ab" />

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ["$","$"], ["\\(","\\)"] ],
        displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno"
      }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>    


  







</head>
  <body>
    <main class="container">
      <section class="about">
        <a href="/" class="iconlink"> 
          <h3> 
            <i class="fas fa-home"></i> HOME 
          </h3> 
        </a>
        <a href="/"> <img src="/assets/portfolio.png" alt="Weikai Mao"></a>
        <h2 id="title"> Weikai Mao </h2>
        <p style="font-size:90%" class="tagline">maoweikai123@outlook.com</p>
        
        <ul class="social"><a href="https://github.com/WalkerMao">
              <li>
                <i class="fab fa-github"></i>
              </li>
            </a><a href="https://www.linkedin.com/in/weikai-mao-000249124">
              <li>
                <i class="fab fa-linkedin"></i>
              </li>
            </a><a href="/wechat-qr-code.html">
              <li>
                <i class="fab fa-weixin"></i>
              </li>
            </a></ul><h3 style="color:gray; font-weight:normal"> Categories: </h3>
            <ul class="post-categories" style="max-width:225px;">
              
                  <a class="post-link" href="/"> <li style="padding: 4px 8px;"> All </li> </a>
              
                  <a class="post-link" href="/categories/cs"> <li style="padding: 4px 8px;"> CS </li> </a>
              
                  <a class="post-link" href="/categories/cv"> <li style="padding: 4px 8px;"> CV </li> </a>
              
                  <a class="post-link" href="/categories/dl"> <li style="padding: 4px 8px;"> DL </li> </a>
              
                  <a class="post-link" href="/categories/fe"> <li style="padding: 4px 8px;"> FE </li> </a>
              
                  <a class="post-link" href="/categories/math"> <li style="padding: 4px 8px;"> Math </li> </a>
              
                  <a class="post-link" href="/categories/ml"> <li style="padding: 4px 8px;"> ML </li> </a>
              
                  <a class="post-link" href="/categories/nlp"> <li style="padding: 4px 8px;"> NLP </li> </a>
              
                  <a class="post-link" href="/categories/stat"> <li style="padding: 4px 8px;"> Stat </li> </a>
              
                  <a class="post-link" href="/categories/杂"> <li style="padding: 4px 8px;"> 杂 </li> </a>
              
            </ul><p>&copy; 2023 </p>

      </section>
      <section class="content">
        <div class="sidebar">
  <ul><li><a href="#optimization">Optimization</a><ul><li><a href="#loss-function">Loss Function</a></li><li><a href="#initialization">Initialization</a><ul><li><a href="#xavier-initialization">Xavier Initialization</a></li><li><a href="#he-initialization">He Initialization</a></li></ul></li><li><a href="#backpropagation">Backpropagation</a></li><li><a href="#gradient-descent-with-momentum">Gradient Descent with Momentum</a></li></ul></li><li><a href="#other-tips">Other Tips</a></li></ul>

</div><div class="post-container" id="viewpoint">
  <a class="post-link" href="/training-nns-optimization.html">
    <h2 class="post-title">Training Neural Networks: Optimization</h2>
  </a>
  <div class="post-meta">
    <div>
      <ul class="post-categories"><a class="post-link" href="/categories/dl"> <li>DL</li> </a></ul>
      <ul class="post-tags"><a class="post-link" href="/tags/optimization"> <li>Optimization</li> </a></ul>
    </div>
    <div class="post-date">
      <i class="icon-calendar"></i>
      Jul 8, 2020
    </div>
  </div>
  <div style="line-height:77%;">
    <br>
  </div>
  <div class="post"><h2 id="optimization">Optimization</h2>

<p>Given a neural network, the goal of the training procedure is to learn parameters $W^{[l]}$ and $b^{[l]}$ for each layer $l$ that minimize the loss function plus regularization term.</p>

<p>We first assume there are $n^{[l-1]}$ hidden units in the hidden layer $l-1$ and $n^{[l]}$ units in the layer $l$. Denote the output of the layer \(l-1\) (it is also the input of the layer \(l\)) as $a^{[l-1]} \in \mathbb{R}^{n^{[l-1]}}$. In the layer \(l\), denote the weight matrix as $W^{[l]} \in \mathbb{R}^{n^{[l]}\times n^{[l-1]}}$, bias vector as $b^{[l]}\in\mathbb{R}^{n^{[l]}}$, activation function as \(\sigma^{[l]}(\cdot)\), then the output of the layer \(l\) is</p>

\[a^{[l]} = \sigma^{[l]}(W^{[l]} a^{[l-1]} + b^{[l]}) \in \mathbb{R}^{n^{[l]}}.\]

<h3 id="loss-function">Loss Function</h3>

<p>The observations for target variable are \(y=(y_1,y_2,\cdots,y_n)^T\in\mathbb{R}^n\), and the corresponding predictions are \(\hat{y}=(\hat{y}_1, \hat{y}_2, \cdots, \hat{y}_n)^T \in \mathbb{R}^n\).</p>

<p>For regression problems, the <strong>L2 loss</strong> is most commonly used:</p>

\[L(\hat{y}_i, y_i) = (y_i - \hat{y}_i)^2, \text{ or } L(\hat{y}, y) = \| \hat{y} - y \|_2^2.\]

<p>For $K$-class classification problems, we usually use <strong>cross-entropy loss</strong> function:</p>

\[L(\hat{p}_i, y_i) = -\sum_{k=1}^{K} \mathbf{1}(y_i \text{ in class } k)\log(\hat{p}_{ik}),\]

<p>where \(\hat{p}_{ik}\) is the estimated probability of $y_i$ belongs to class $k$, and \(\mathbf{1}(\cdot)\) is the indicator function. The estimated probabilities \(\hat{p}_{ik}\) is usually calculated by softmax function: \(\hat{p}_{ik} = \frac{e^{z_{ik}}}{\sum _{k=1}^{K}e^{z_{ik}}}\).</p>

<p>For binary classification, the cross-entropy loss is</p>

\[L(\hat{p}_i, y_i) = -y_i\log(\hat{p}_i) - (1-y_i)\log(1-\hat{p}_i),\]

<p>where $y_i\in{0,1}$, $\hat{p}_i$ is the estimated probability of $y_i=1$.</p>

<h3 id="initialization">Initialization</h3>

<p>If all weights are initialized to the same value, the activations of neurons in each layer are same, and the gradients of weights in each layer are same, it follows that the network has the same effectiveness as a network that has only single neuron in each layer. In a special case that all weights are initialized to \(0\) and we use the activation function that \(\sigma(0)=0\) (such as Tanh, ReLU), then all activations and gradients are \(0\), and the weights cannot be updated through gradient descent. Thus, the weights in same layer should be initialized differently (usually randomly) to break symmetry during training.</p>

<p>If the weights in a network start too small/large, then the backpropagated gradient signal shrinks/grows as it passes through and it may lead to vanishing/exploding gradients, and that can result in divergence/slow-down in the training of the network.</p>

<p>The appropriate initialization that follows the rules below guarantees no exploding/vanishing (either forward or backward) signal.</p>

<ol>
  <li>The mean (expectation) of the activations should be same and be zero. i.e. \(E(a^{[l-1]}) = E(a^{[l]})=0\);</li>
  <li>The variance of the activations should stay the same across every layer. i.e. \(\text{Var}(a^{[l-1]}) = \text{Var}(a^{[l]})\).</li>
</ol>

<h4 id="xavier-initialization">Xavier Initialization</h4>

<p>Xavier initialization was introduced by Glorot Xavier, and it works for activation functions like sigmoid, tanh and linear etc..</p>

<p>That is, for every layer $l$, we initialize</p>

\[W^{[l]} \sim N\left(0,\frac{1}{n^{[l-1]}}\right) \text{ or } N\left(0,\frac{2}{n^{[l-1]} + n^{[l]}}\right), \ b^{[l]}=0.\]

<p>We first derive the Xavier initialization through the forward propagation. Now the purpose of the initialization is to avoid the vanishing or exploding of the forward propagated signal.</p>

<p>Based on some assumptions (assume the activation function is tanh. etc.), we can show that</p>

\[\text{Var}(a^{[l]}) = n^{[l-1]} \text{Var}(W^{[l]}) \cdot \text{Var}(a^{[l-1]}).\]

<p>It follows that setting \(\text{Var}(W^{[l]}) = 1/n^{[l-1]}\) leads to \(\text{Var}(a^{[l-1]}) = \text{Var}(a^{[l]})\).</p>

<p>At every layer, we can link this layer’s variance to the input layer’s variance:</p>

\[\begin{align}
\text{Var}(a^{[l]}) &amp;= n^{[l-1]} \text{Var}(W^{[l]}) \cdot \text{Var}(a^{[l-1]}) \\
&amp;= n^{[l-1]} \text{Var}(W^{[l]}) \cdot n^{[l-2]} \text{Var}(W^{[l-1]}) \cdot \text{Var}(a^{[l-2]}) \\
&amp;= \cdots \\
&amp;= \left[ \prod_{s=1}^l n^{[s-1]} \text{Var}(W^{[s]}) \right] \cdot \text{Var}(x).
\end{align}\]

<p>Then we have the following three cases:</p>

\[\begin{align}
&amp;\text{For all } s=1,\cdots,l, \\
&amp;n^{[s-1]} \text{Var}(W^{[s]}) 
\begin{cases}
&lt;1 \implies \text{Var}(a^{[l]}) \ll \text{Var}(x); \\
=1 \implies \text{Var}(a^{[l]}) = \text{Var}(x); \\
&gt;1 \implies \text{Var}(a^{[l]}) \gg \text{Var}(x). \\
\end{cases}
\end{align}\]

<p>Since the mean is zero (i.e. \(E(a^{[l]})=0\)), low variance (i.e. \(\text{Var}(a^{[l]})\) is small) means that the activations \(a^{[l]}\) would be very close to zero (forward signal vanishing), and high variance means that the activations would be very large in absolute value (forward signal exploding).</p>

<p>Throughout the justification, we worked on activations computed during the forward propagation, and we conclude that, to avoid the vanishing or exploding of the forward propagated signal, we initialize</p>

\[\text{Var}(W^{[l]}) = \frac{1}{n^{[l-1]}}.\]

<p>The same result can be derived for the backpropagated gradients. Doing so, you will see that in order to avoid the vanishing or exploding gradient problem, we should initialize</p>

\[\text{Var}(W^{[l]}) = \frac{1}{n^{[l]}}.\]

<p>As a compromise, we can roughly took the harmonic mean of the two:</p>

\[\text{Var}(W^{[l]}) = \frac{2}{n^{[l-1]} + n^{[l]}}.\]

<h4 id="he-initialization">He Initialization</h4>

<p>He initialization was introduced by Kaiming He, and it works for activation functions like ReLU and leaky ReLU etc..</p>

<p>That is, for every layer $l$, we initialize</p>

\[W^{[l]} \sim N\left(0,\frac{2}{n^{[l-1]}}\right), \ b^{[l]}=0.\]

<p>For activation functions like ReLU and leaky ReLU etc., this initialization avoid the vanishing or exploding of both the forward propagated signal and backpropagated gradients.</p>

<h3 id="backpropagation">Backpropagation</h3>

<p>We usually use gradient descent to optimize the objective function $\text{Obj}(w)$. For weight, we have</p>

\[w_t = w_{t-1} - \eta \frac{\partial \text{Obj}(w_{t-1})}{\partial w_{t-1}},\]

<p>where $\eta$ is the step-size (learning rate).</p>

<p>The solution to computing the gradient is an algorithm called error <strong>backpropagation</strong>, which use the chain rules in calculus. (Note: the regularization term is not considered in the pictures below.)</p>

<div style="text-align: center"> <img src="../../../pictures/backpropagation.png" alt="backpropagation" style="zoom: 40%;" /> </div>

<p>Here let’s take an example. Say there is a feed forward neural network for regression, as shown in the picture below.</p>

<div style="text-align: center"> <img src="../../../pictures/neural_network_example_2.png" alt="Grid Search and Random Search" style="zoom:25%;" /> </div>

<p>Suppose the activation function \(\sigma(\cdot)\) for hidden layer is sigmoid, and the activation function for output layer is identity function. Denote the input vector as \(x\in\mathbb{R}^{p}\), the target as \(y\in\mathbb{R}\), the output scalar as \(\hat{y}\in\mathbb{R}\), the weights of hidden layer as $W\in\mathbb{R}^{p \times q}$, the weights of output layer as \(V\in\mathbb{R}^{q}\), the loss function as \(L(\hat{y},y)=(\hat{y}-y)^2\). This network can be expressed mathematically as</p>

\[\hat{y} = \sigma(x^TW) \cdot V,\]

<p>where \(x\) is the input of the hidden layer, \(\sigma(x^TW)\) is the output of the hidden layer and is also the input of the output layer, and \(\hat{y}\) is the output of the output layer.</p>

<p>Denote \(w_r\in\mathbb{R}^p\) as the \(r\)-th column vector of \(W\), \(v_r\in\mathbb{R}\) as the $r$-th entry of \(V\), and \(\mathbf{1}\) as an all ones vector. We have \(W = (w_1,w_2,\cdots,w_q)\) and \(V=(v_1,v_2,\cdots,v_q)^T\), then the network can be expressed in summation form as</p>

\[\hat{y} = \sigma(x^TW) \cdot V = \Big( \sigma(x^Tw_1), \cdots, \sigma(x^Tw_q) \Big) \cdot V = \sum_{r=1}^q \sigma(x^Tw_r) v_r.\]

<p>Now let’s compute the gradients for \(v_r\) and \(w_r\):</p>

\[\begin{align}
&amp; \frac{\partial L(\hat{y},y)}{\partial v_r} = \frac{\partial L(\hat{y},y)}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial v_r} = 2(\hat{y}-y) \cdot \sigma(w_r^Tx), \\
&amp; \frac{\partial L(\hat{y},y)}{\partial w_r} = \frac{\partial L(\hat{y},y)}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial \sigma(x^Tw_r)} \frac{\partial \sigma(x^Tw_r)}{x^Tw_r} \frac{\partial x^Tw_r}{w_r} = 2(\hat{y}-y) v_r \cdot \sigma(x^Tw_r) [1 - \sigma(x^Tw_r)] \cdot x.
\end{align}\]

<p>Or directly the gradients for \(V\) and \(W\):</p>

\[\begin{align}
&amp; \frac{\partial L(\hat{y},y)}{\partial V} = \frac{\partial L(\hat{y},y)}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial V} = 2(\hat{y}-y) \cdot \sigma(W^Tx), \\
&amp; \frac{\partial L(\hat{y},y)}{\partial W} = \frac{\partial L(\hat{y},y)}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial \sigma(x^TW)} \frac{\partial \sigma(x^TW)}{x^TW} \frac{\partial x^TW}{W} = 2(\hat{y}-y) x \cdot \big[ V^T \odot \sigma(x^TW) \odot [\mathbf{1} - \sigma(x^TW)] \big].
\end{align}\]

<h3 id="gradient-descent-with-momentum">Gradient Descent with Momentum</h3>

<p>The basic idea of momentum is to compute an moving average of the gradients, and use that average gradient to update the parameters.</p>

<p>At the \(t\)-th step of gradient update of weights, we have \(w_t = w_{t-1} - \eta v_{w_{t}}\). In standard gradient descent (GD), we set \(v_{w_{t}}\) as \(\frac{\partial\text{Obj}(w_{t-1})}{\partial w_{t-1}}\). But in the gradient descent (GD) with momentum, we set 
\(v_{w_{t}} = \beta v_{w_{t-1}} + (1-\beta) \cdot \frac{\partial\text{Obj}(w_{t-1})}{\partial w_{t-1}}.\)</p>

<p>The hyperparameter \(\beta\) is usually set to \(0.9\) or a similar value.</p>

<p>In the GD with momentum, if the history gradients \(\frac{\partial\text{Obj}(w_{t'})}{\partial w_{t'}^{(j)}}\), (\(t'=t-1,t-2,\cdots\)) in dimension \(j\) point in the same direction, then the parameter updating amplitude \(\mid v_{w_t^{(j)}} \mid\) is increased through the moving average, compared to the size of the current gradient \(\mid\frac{\partial\text{Obj}(w_{t-1}^{(j)})}{\partial w_{t-1}^{(j)}}\mid\) in standard GD. Otherwise, if the history gradients change directions, the parameter updating amplitude is decreased through the moving average. As a result, GD with momentum accelerates convergence and reduces oscillation.</p>

<div align="center">
<figure>
<img src="../../../pictures/gradient-descent-with-momentum.png" alt="gradient-descent-with-momentum" style="zoom: 80%;" />
<figcaption style="font-size: 80%;"> 
	Gradient deacent with momentum accelerates convergence and reduces oscillation.
    (<a href="https://programmersought.com/article/63854417042/">Figure source</a>) </figcaption>
</figure>
</div>

<p>As shown in the plot above, suppose the parameter in horizontal direction is \(w^{(1)}\) and that in vertical direction is \(w^{(2)}\). The updating amplitude \(\mid v_{w_t^{(1)}} \mid\) is increased compared to the size of the current gradient \(\mid\frac{\partial\text{Obj}(w_{t-1}^{(1)})}{\partial w_{t-1}^{(1)}}\mid\), thus the GD with momentum accelerates the convergence of \(w^{(1)}\). On the other hand, the updating amplitude \(\mid v_{w_t^{(2)}} \mid\) is decreased compared to the size of the current gradient \(\mid\frac{\partial\text{Obj}(w_{t-1}^{(2)})}{\partial w_{t-1}^{(2)}}\mid\), thus the GD with momentum reduces the oscillation of \(w^{(2)}\).</p>

<h2 id="other-tips">Other Tips</h2>

<p>Batch Gradient Descent (BGD) is usually computationally expensive, thus we use Stochastic Gradient Descent (SGD) or Mini-Batch Gradient Descent (MBGD).</p>

<p>In SGD, at every gradient updating step, we take only one example to calculate the gradient, and update the parameters.</p>

<p>In MBGD, we use a batch of a fixed number of training examples which is less than the actual dataset and call it a mini-batch. We first create the mini-batches of fixed size, at every epoch, we iterate over every mini-batches, and calculate the mean gradient of the mini-batch, and update the parameters.</p>

<p>However, SGD and MBGD are more sensitive to learning rate and initialization. A large learning rate or an improper initialization may lead to vanishing gradient or exploding gradient.</p>

<p><br /></p>

<p><strong>References:</strong></p>

<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.</p>

<p>Glorot, Xavier, and Yoshua Bengio. “Understanding the difficulty of training deep feedforward neural networks.” <em>Proceedings of the thirteenth international conference on artificial intelligence and statistics</em>. 2010.</p>

<p>He, Kaiming, et al. “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.” <em>Proceedings of the IEEE international conference on computer vision</em>. 2015.</p>

<p>Katanforoosh &amp; Kunin. “<a href="https://www.deeplearning.ai/ai-notes/initialization/">Initializing neural networks</a>”. <em>DeepLearning.AI</em>. 2019.</p>

<p><a href="http://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a>.</p>

<p>Ng, Andrew. “<a href="https://www.coursera.org/lecture/deep-neural-network/gradient-descent-with-momentum-y0m1f">Gradient descent with momentum</a>”. <em>DeepLearning.AI</em>.</p>


  </div>

  <br> </br>
  <p><font color="grey" size="4"> Comments </font></p>
  <HR color=#D1D0CE SIZE=10>

<div id="disqus_thread"></div>

<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://walkermao.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


                            

  
</div>

      </section>
    </main><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178951885-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178951885-1');
</script>

    
    <div id="back-top">
      <a href="javascript:void(0);" onclick="topFunction()" title="Back to top"> </a>
    </div>

  </body>
</html>

<script src = "/assets/js/scroll_into_view.js"></script>
<script src = "/assets/js/back_to_top.js"></script>