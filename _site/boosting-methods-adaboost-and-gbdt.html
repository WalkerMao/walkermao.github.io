<!DOCTYPE html>
<html lang="en">
  <head>
    <script src="https://kit.fontawesome.com/123ecac47c.js" crossorigin="anonymous"></script><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="shortcut icon" type="image/png" href="/assets/portfolio.png">
<title>Boosting Methods: AdaBoost and GBDT</title>
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Boosting Methods: AdaBoost and GBDT | Weikai’s blog.</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Boosting Methods: AdaBoost and GBDT" />
<meta name="author" content="Weikai Mao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In Boosting, we construct a strong predictive algorithm by iteratively layering weak predictive algorithms (weak learners)." />
<meta property="og:description" content="In Boosting, we construct a strong predictive algorithm by iteratively layering weak predictive algorithms (weak learners)." />
<link rel="canonical" href="http://localhost:4000/boosting-methods-adaboost-and-gbdt.html" />
<meta property="og:url" content="http://localhost:4000/boosting-methods-adaboost-and-gbdt.html" />
<meta property="og:site_name" content="Weikai’s blog." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-14T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Boosting Methods: AdaBoost and GBDT" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Weikai Mao"},"dateModified":"2020-03-14T00:00:00+08:00","datePublished":"2020-03-14T00:00:00+08:00","description":"In Boosting, we construct a strong predictive algorithm by iteratively layering weak predictive algorithms (weak learners).","headline":"Boosting Methods: AdaBoost and GBDT","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/boosting-methods-adaboost-and-gbdt.html"},"url":"http://localhost:4000/boosting-methods-adaboost-and-gbdt.html"}</script>
<!-- End Jekyll SEO tag -->


<meta name="google-site-verification" content="wXp8C1QlYKCpKxfXyFfQXEv9l5fJvcOi53ofYmOcaSA" />
<meta name="msvalidate.01" content="97F0BB32D312B808156DE357EA8474D3" />
<meta name="yandex-verification" content="690106a82d8966ab" />

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ["$","$"], ["\\(","\\)"] ],
        displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno"
      }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>    


  







</head>
  <body>
    <main class="container">
      <section class="about">
        <a href="/" class="iconlink"> 
          <h3> 
            <i class="fas fa-home"></i> HOME 
          </h3> 
        </a>
        <a href="/"> <img src="/assets/portfolio.png" alt="Weikai Mao"></a>
        <h2 id="title"> Weikai Mao </h2>
        <p style="font-size:90%" class="tagline">maoweikai123@outlook.com</p>
        
        <ul class="social"><a href="https://github.com/WalkerMao">
              <li>
                <i class="fab fa-github"></i>
              </li>
            </a><a href="https://www.linkedin.com/in/weikai-mao-000249124">
              <li>
                <i class="fab fa-linkedin"></i>
              </li>
            </a><a href="/wechat-qr-code.html">
              <li>
                <i class="fab fa-weixin"></i>
              </li>
            </a></ul><h3 style="color:gray; font-weight:normal"> Categories: </h3>
            <ul class="post-categories" style="max-width:225px;">
              
                  <a class="post-link" href="/"> <li style="padding: 4px 8px;"> All </li> </a>
              
                  <a class="post-link" href="/categories/cs"> <li style="padding: 4px 8px;"> CS </li> </a>
              
                  <a class="post-link" href="/categories/cv"> <li style="padding: 4px 8px;"> CV </li> </a>
              
                  <a class="post-link" href="/categories/dl"> <li style="padding: 4px 8px;"> DL </li> </a>
              
                  <a class="post-link" href="/categories/fe"> <li style="padding: 4px 8px;"> FE </li> </a>
              
                  <a class="post-link" href="/categories/math"> <li style="padding: 4px 8px;"> Math </li> </a>
              
                  <a class="post-link" href="/categories/ml"> <li style="padding: 4px 8px;"> ML </li> </a>
              
                  <a class="post-link" href="/categories/nlp"> <li style="padding: 4px 8px;"> NLP </li> </a>
              
                  <a class="post-link" href="/categories/stat"> <li style="padding: 4px 8px;"> Stat </li> </a>
              
                  <a class="post-link" href="/categories/杂"> <li style="padding: 4px 8px;"> 杂 </li> </a>
              
            </ul><p>&copy; 2023 </p>

      </section>
      <section class="content">
        <div class="sidebar">
  <ul><li><a href="#adaboost">AdaBoost</a></li><li><a href="#boosting-fits-an-additive-model">Boosting Fits an Additive Model</a></li><li><a href="#boosting-trees">Boosting Trees</a></li><li><a href="#gradient-boosting">Gradient Boosting</a></li></ul>

</div><div class="post-container" id="viewpoint">
  <a class="post-link" href="/boosting-methods-adaboost-and-gbdt.html">
    <h2 class="post-title">Boosting Methods: AdaBoost and GBDT</h2>
  </a>
  <div class="post-meta">
    <div>
      <ul class="post-categories"><a class="post-link" href="/categories/ml"> <li>ML</li> </a></ul>
      <ul class="post-tags"><a class="post-link" href="/tags/ensemble learning"> <li>Ensemble learning</li> </a><a class="post-link" href="/tags/ml models"> <li>ML models</li> </a></ul>
    </div>
    <div class="post-date">
      <i class="icon-calendar"></i>
      Mar 14, 2020
    </div>
  </div>
  <div style="line-height:77%;">
    <br>
  </div>
  <div class="post"><p>In Boosting, we construct a strong predictive algorithm by iteratively layering weak predictive algorithms (weak learners).</p>

<p>A weak learner is one whose error rate is only slightly better than random guessing. The purpose of boosting is to sequentially apply the weak predictive algorithm to repeatedly modified versions of the data, thereby producing a sequence of weak learners: $f_m(x), m = 1, 2, \cdots, M$.</p>

<p>The predictions from all of them are then combined through a weighted majority vote to produce the final prediction:</p>

\[f^{(M)}(x) = \sum_{m=1}^{M} \eta_m f_m(x).\]

<p>Here $\eta_1,\eta_2,\cdots ,\eta_M$ are computed by the boosting algorithm, and weight the contribution of each respective $f_m(x)$. Their effect is to give higher influence to the more accurate learners in the sequence. The data modifications at each boosting step consist of applying weights $w_1,w_2,\cdots, w_N$ to each of the training observations \((x_i,y_i),i=1,2,\cdots,N\).</p>

<h2 id="adaboost">AdaBoost</h2>

<p>AdaBoost, short for Adaptive Boosting, is a basic boosting methods. It can be implemented for both regression and classification problems.</p>

<div style="text-align: center">
<img src="../../../pictures/adaboost.png" alt="adaboost.png" style="zoom:70%;" />
</div>

<p>The algorithm below is the AdaBoost.M1 algorithm that used for classification problem.</p>

<p><strong>Algorithm. AdaBoost.M1.</strong></p>

<p>[1] Initialize the observation weights \({w_i}_1 = \frac{1}{N}, i=1,2,\cdots,N\).</p>

<p>[2] For $m = 1$ to $M$, do:</p>

<p>  (a) Fit a classifier $f_m(x)$ to the training data using weights $w_i$;</p>

<p>  (b) Compute \(\text{err}_m = \frac{\sum_{i=1}^{N} {w_i}_m \mathbf{1}(y_i \neq f_m(x_i))}{\sum_{i=1}^{N} {w_i}_m}\);</p>

<p>  (c) Compute \(\alpha_m = \log \big( \frac{1 − \text{err}_m}{\text{err}_m} \big)\);</p>

<p>  (d) Update sample weight \({w_i}_{m+1} = {w_i}_m \cdot \exp\left[\alpha_m \cdot \mathbf{1}(y_i \neq f_m (x_i))\right]\) for all \(i\).</p>

<p>[3] Output $f^{(M)}(x) = \sum_{m=1}^{M} \eta_m f_m(x).$</p>

<h2 id="boosting-fits-an-additive-model">Boosting Fits an Additive Model</h2>

<p>Boosting is a way of fitting an additive expansion in a set of elementary “basis” functions. The philosophy is like Taylor expansion.</p>

<p>Basis function expansions take the form</p>

\[f(x) = \sum_{m=1}^{M} \eta_m b(x; \theta_m),\]

<p>where $\eta_m , m = 1, 2, . . . , M$ are the expansion coefficients, and $b(x; \theta) ∈ \mathbb{R}$ are usually simple basis functions of the multivariate argument $x$, characterized by a set of parameters $\mu$.</p>

<p>In the algorithm AdaBoost.M1. The basis functions are the individual classifiers \(f_m(x) \in \{−1, 1\}\).</p>

<p><strong>Algorithm: Forward Stagewise Additive Modeling.</strong></p>

<p>[1] Initialize $f^{(0)}(x) = 0$.</p>

<p>[2] For $m = 1$ to $M$, do:</p>

<p>  (a) Compute \((\eta_m, \theta_m) = \operatorname*{argmin}_{\eta,\theta} \sum_{i=1}^{N} L\big(y_i, f_{m−1}(x_i) + \eta b(x_i;\theta)\big)\).</p>

<p>  (b) Set $f^{(m)}(x) = f^{(m−1)}(x) + \eta_mb(x;\theta_m)$.</p>

<p>[3] Output $f^{(M)}(x)$.</p>

<p>Suppose $\theta$ is of dimension $p_{\theta}$. In $m$-th step, we need to do optimization with $1+p_{\theta}$ parameters ($\eta_m$ is $1$ dimension and $\theta_m$ is $p_{\theta}$). At the end, $f^{(M)}(x)$ has $M(1+p_{\theta})$ parameters. That is much easier than we do optimization with $M(1+p_{\theta})$ parameters simultaneously: \((\eta, \theta) = \operatorname*{argmin}_{\eta,\theta} \sum_{i=1}^{N} L\big(y_i, f^{(M)}(x_i)\big)\).</p>

<p>AdaBoost.M1 is equivalent to forward stagewise additive modeling using the exponential loss function $L(y, f (x)) = \exp(-y f (x))$. Exponential loss is suitable for classification but not for regression.</p>

<p><strong>Gradient boosting and XGBoost use the gradient descent and Newton’s method respectively to do the step [2] (a) in the algorithm above, and minimize the loss (or objective) function iteratively.</strong></p>

<h2 id="boosting-trees">Boosting Trees</h2>

<p>Tree ensemble methods (boosting: e.g. GBM, XGBoost, bagging: e.g. random forest) are widely used. Almost half of data mining competition are won by using some variants of tree ensemble methods.</p>

<p>As for CART, a constant $\mu_j$ is assigned to each region $R_j$ and the predictive rule is $x ∈ R_j ⇒ f(x)=\mu_j$.</p>

<p>Thus a tree can be expressed as the additive model form:</p>

\[\text{Tree}(x;Θ) = \sum_{j=1}^{T} \mu_j \mathbf{1}(x \in R_j),\]

<p>with parameters \(Θ = \{R_j , \mu_j\}^T_1\), where \(T\) is the number of leaf nodes and is usually treated as a meta-parameter.</p>

<p>The parameters are found by minimizing the training loss</p>

\[\hat{Θ} = \operatorname*{argmin}_{Θ} \sum_{j=1}^{T} \sum_{x_i \in R_j} L(y_i,\mu_j),\]

<p>It is useful to divide the optimization problem into two parts:</p>

<ol>
  <li>Finding \(R_j\): A typical strategy is to use a greedy, top-down recursive partitioning algorithm to find the \(R_j\).</li>
  <li>Finding \(\mu_j\) given \(R_j\): Given the \(R_j\), often \(\hat{\mu}_j = \frac{1}{\mid R_j\mid} \sum_{x_i \in R_j} y_i\) , which is the mean of the \(y_i\)’s falling in region \(R_j\).</li>
</ol>

<p>The <strong>boosted tree</strong> model is a sum of such trees,</p>

\[f^{(M)}(x) = \sum_{m=1}^{M} \text{Tree}(x; Θ_m),\]

<p>induced in a forward stagewise manner (Algorithm: Forward Stagewise Additive Modeling). At each step in the forward stagewise procedure one must solve</p>

\[\hat{Θ}_m = \operatorname*{argmin}_{Θ_m} \sum_{i=1}^{N} L \big(y_i, f^{(m-1)}(x_i) + \text{Tree}(x_i;\Theta_m) \big)\]

<p>for \(\Theta_m = \{ R_{jm}, \mu_{jm} \}_{j=1}^{T_m}\) of the next tree, given the current model $f^{(m-1)}(x)$.</p>

<p>For $m$-th tree, given the regions $R_{jm}$’s ($j=1,\cdots,T_m$), finding the optimal constants $\mu_{jm}$’s in each region is straightforward and easy. However, finding the regions $R_{jm}$’s for the $m$-th tree is difficult. For squared-error loss, a special case, \(\hat{\Theta}_m\) is simply the parameters of the regression tree that best predicts the current residuals \(y_i - f^{(m-1)}(x_i), (i=1, \cdots,N)\).</p>

<p>Note that in boosting methods, especially boosting trees, we can also use bootstrap sample and subset of features like that in random forest to reduce overfitting.</p>

<h2 id="gradient-boosting">Gradient Boosting</h2>

<p>Gradient boosting is based on gradient descent. Instead of minimizing, gradient boosting use the gradient descent to do the step [2] (a) in the algorithm Forward Stagewise Additive Modeling.</p>

<p>We induce a weak regressor or classifier $f_m(x)$ (e.g. a tree $\text{Tree}(x;Θ_m)$) at the $m$-th iteration to fit the negative gradient, which means the predictions (a vector with dimension $n \times 1$) by weak model $f_m(x)$ are as close as possible to the negative gradient.</p>

<p>We want to minimize $L(y_i, f(x_i)), i=1,…N$, where $f$ is the independent variable.</p>

<p>Recall Taylor expansion: $f(x+\Delta x) \approx f(x)+f’(x)\Delta x$. at $m$-th step, we want to minimize the loss function</p>

\[\sum_{i=1}^{N} L\Big(y_i,f^{(m-1)}(x_i)+f_m(x_i)\Big) \approx \sum_{i=1}^{N} \Big[L(y_i, f^{(m-1)}(x_i)) + g_{im} f_m(x_i) \Big].\]

<p>Here \(f_m(x_i)\) should be $-g_{im}$, and  \(g_{im} = \frac{∂L(y_i,f^{(m-1)}(x_i))}{∂f^{(m-1)}(x_i)}\) is the gradient.</p>

<p>Thus, we have the iterative formula:</p>

\[f^{(m)}(x_i) = f^{(m-1)}(x_i) + \eta_m f_{m}(x_i) = f^{(m-1)}(x_i) - \eta_m g_{im},\]

<p>where $\eta_m$ is the step size.</p>

<p>If $L(y_i, f^{(m-1)}(x_i))=\frac{1}{2}[y_i-f^{(m-1)}(x_i)]^2$, then the gradient $\frac{∂L(y_i,f^{(m-1)}(x_i))}{∂f^{(m-1)}(x_i)}=f^{(m-1)}(x_i)-y_i$, which is the residual $r_{im}$.</p>

<p>Gradient boosting decision tree (GBDT) is a method when decision tree is applied as the weak learner in gradient boosting.</p>

<p><strong>Algorithm: Gradient Boosting Decision Tree (GBDT) Algorithm.</strong></p>

<p>[1] Initialize \(f^{(0)}(x) = \operatorname*{argmin}_\mu \sum_{i=1}^N L(y_i,\mu)\).</p>

<p>[2] For $m = 1$ to $M$, do:</p>

<p>  (a) For $i = 1,2,…,N$ compute</p>

\[g_{im} = \frac{∂L(y_i,f^{(m-1)}(x_i))}{∂f^{(m-1)}(x_i)}.\]

<p>  (b) Fit a <strong>regression tree</strong> \(f_m(x)\) to the targets $-g_{im}$’s to get terminal regions $R_{jm},j=1,2,..,T_m$.</p>

<p>  (c) For $j = 1,2,…,T_m$, compute the leaf node prediction $\mu_{jm}$. Now we get a new tree</p>

\[f_m(x) = \text{Tree}(x; \Theta_m) = \sum_{j=1}^{T_m} \mu_{jm}\mathbf{1}(x \in R_{jm}).\]

<p>  (d) Compute step size \(\eta_m = \operatorname*{argmin}_{\eta} \sum_{i=1}^N L\big(y_i,f^{(m-1)}(x_i)+ \eta f_m(x_i) \big)\).</p>

<p>  (e) Update</p>

\[f^{(m)}(x) = f^{(m-1)}(x) + \eta_m f_m(x),\]

<p>    where $f_m(x) = \sum_{j=1}^{T_m} \mu_{jm} \mathbf{1}(x \in R_{jm})$ is a new tree ($m$-th tree).</p>

<p>[3] Output $\hat{f}(x) = f^{(M)}(x)$.</p>

<p><br /></p>

<p><strong>References:</strong></p>

<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. <em>The elements of statistical learning</em>. Vol. 1. No. 10. New York: Springer series in statistics, 2001.</p>

<p>张, 凌寒. “GBDT与XGBoost解析及应用.” <em>腾讯云</em>, 13 May 2019, cloud.tencent.com/developer/article/1424251.</p>

  </div>

  <br> </br>
  <p><font color="grey" size="4"> Comments </font></p>
  <HR color=#D1D0CE SIZE=10>

<div id="disqus_thread"></div>

<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://walkermao.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


                            

  
</div>

      </section>
    </main><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178951885-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178951885-1');
</script>

    
    <div id="back-top">
      <a href="javascript:void(0);" onclick="topFunction()" title="Back to top"> </a>
    </div>

  </body>
</html>

<script src = "/assets/js/scroll_into_view.js"></script>
<script src = "/assets/js/back_to_top.js"></script>