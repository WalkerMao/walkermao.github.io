<!DOCTYPE html>
<html lang="en">
  <head>
    <script src="https://kit.fontawesome.com/123ecac47c.js" crossorigin="anonymous"></script><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="shortcut icon" type="image/png" href="/assets/portfolio.png">
<title>Word Vectors (Part 1)</title>
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Word Vectors (Part 1) | Weikai’s blog.</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Word Vectors (Part 1)" />
<meta name="author" content="Weikai Mao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Vector Semantics" />
<meta property="og:description" content="Vector Semantics" />
<link rel="canonical" href="http://localhost:4000/word-vectors-part1.html" />
<meta property="og:url" content="http://localhost:4000/word-vectors-part1.html" />
<meta property="og:site_name" content="Weikai’s blog." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-22T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Word Vectors (Part 1)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Weikai Mao"},"dateModified":"2020-06-22T00:00:00+08:00","datePublished":"2020-06-22T00:00:00+08:00","description":"Vector Semantics","headline":"Word Vectors (Part 1)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/word-vectors-part1.html"},"url":"http://localhost:4000/word-vectors-part1.html"}</script>
<!-- End Jekyll SEO tag -->


<meta name="google-site-verification" content="wXp8C1QlYKCpKxfXyFfQXEv9l5fJvcOi53ofYmOcaSA" />
<meta name="msvalidate.01" content="97F0BB32D312B808156DE357EA8474D3" />
<meta name="yandex-verification" content="690106a82d8966ab" />

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ["$","$"], ["\\(","\\)"] ],
        displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno"
      }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>    


  







</head>
  <body>
    <main class="container">
      <section class="about">
        <a href="/" class="iconlink"> 
          <h3> 
            <i class="fas fa-home"></i> HOME 
          </h3> 
        </a>
        <a href="/"> <img src="/assets/portfolio.png" alt="Weikai Mao"></a>
        <h2 id="title"> Weikai Mao </h2>
        <p style="font-size:90%" class="tagline">maoweikai123@outlook.com</p>
        
        <ul class="social"><a href="https://github.com/WalkerMao">
              <li>
                <i class="fab fa-github"></i>
              </li>
            </a><a href="https://www.linkedin.com/in/weikai-mao-000249124">
              <li>
                <i class="fab fa-linkedin"></i>
              </li>
            </a><a href="/wechat-qr-code.html">
              <li>
                <i class="fab fa-weixin"></i>
              </li>
            </a></ul><h3 style="color:gray; font-weight:normal"> Categories: </h3>
            <ul class="post-categories" style="max-width:225px;">
              
                  <a class="post-link" href="/"> <li style="padding: 4px 8px;"> All </li> </a>
              
                  <a class="post-link" href="/categories/cs"> <li style="padding: 4px 8px;"> CS </li> </a>
              
                  <a class="post-link" href="/categories/cv"> <li style="padding: 4px 8px;"> CV </li> </a>
              
                  <a class="post-link" href="/categories/dl"> <li style="padding: 4px 8px;"> DL </li> </a>
              
                  <a class="post-link" href="/categories/fe"> <li style="padding: 4px 8px;"> FE </li> </a>
              
                  <a class="post-link" href="/categories/math"> <li style="padding: 4px 8px;"> Math </li> </a>
              
                  <a class="post-link" href="/categories/ml"> <li style="padding: 4px 8px;"> ML </li> </a>
              
                  <a class="post-link" href="/categories/nlp"> <li style="padding: 4px 8px;"> NLP </li> </a>
              
                  <a class="post-link" href="/categories/stat"> <li style="padding: 4px 8px;"> Stat </li> </a>
              
                  <a class="post-link" href="/categories/杂"> <li style="padding: 4px 8px;"> 杂 </li> </a>
              
            </ul><p>&copy; 2023 </p>

      </section>
      <section class="content">
        <div class="sidebar">
  <ul><li><a href="#vector-semantics">Vector Semantics</a></li><li><a href="#words-and-vectors">Words and Vectors</a><ul><li><a href="#vectors-and-documents">Vectors and Documents</a></li><li><a href="#words-as-vectors">Words as Vectors</a></li><li><a href="#cosine-for-measuring-similarity">Cosine for Measuring Similarity</a></li></ul></li><li><a href="#tf-idf">TF-IDF</a></li></ul>

</div><div class="post-container" id="viewpoint">
  <a class="post-link" href="/word-vectors-part1.html">
    <h2 class="post-title">Word Vectors (Part 1)</h2>
  </a>
  <div class="post-meta">
    <div>
      <ul class="post-categories"><a class="post-link" href="/categories/nlp"> <li>NLP</li> </a></ul>
      <ul class="post-tags"></ul>
    </div>
    <div class="post-date">
      <i class="icon-calendar"></i>
      Jun 22, 2020
    </div>
  </div>
  <div style="line-height:77%;">
    <br>
  </div>
  <div class="post"><h2 id="vector-semantics">Vector Semantics</h2>

<p>The intuition is that perhaps there exists some multi-dimensional semantic space that is sufficient to encode all semantics of our words. Each dimension would encode some meaning. For instance, semantic dimensions might indicate tense (past vs. present vs. future), count (singular vs. plural), and gender (masculine vs. feminine).</p>

<p>Vectors for representing words are generally called embeddings, because the word is embedded in a particular vector space. <strong>Vector semantic models are extremely practical because they can be learned automatically from text without any complex labeling or supervision.</strong> Vector models of meaning are now the standard way to represent the meaning of words in NLP.</p>

<h2 id="words-and-vectors">Words and Vectors</h2>

<p>Vector or distributional models of meaning are generally based on a <strong>co-occurrence matrix</strong>, a way of representing how often words co-occur. However, these vectors are usually very sparse.</p>

<h3 id="vectors-and-documents">Vectors and Documents</h3>

<p>In a <strong>term-document matrix</strong>, each row represents a word in the vocabulary and each column represents a document, which means a word vector is a row vector in the matrix and a document vector is a column. The term-document matrix has \(\mid\mathcal{V}\mid\) rows (one for each word type in the vocabulary) and \(\mid\mathcal{D}\mid\) columns (one for each document in the collection), where \(\mid\mathcal{V}\mid\) is the number of words in the vocabulary set $\mathcal{V}$, and \(\mid\mathcal{D}\mid\) is the number of documents in the documents set \(\mathcal{D}\).</p>

<p>As shown in the table below, The term-document matrix for four words in four Shakespeare plays. Each cell contains the number of times the (row) word occurs in the (column) document.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>As You Like It</th>
      <th>Twelfth Night</th>
      <th>Julius Caesar</th>
      <th>Henry V</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>battle</td>
      <td>1</td>
      <td>0</td>
      <td>7</td>
      <td>13</td>
    </tr>
    <tr>
      <td>good</td>
      <td>114</td>
      <td>80</td>
      <td>62</td>
      <td>89</td>
    </tr>
    <tr>
      <td>fool</td>
      <td>36</td>
      <td>58</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <td>wit</td>
      <td>20</td>
      <td>15</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<p>Term-document matrices were originally defined as a means of finding similar documents for the task of document <strong>information retrieval</strong>. Information retrieval (IR) is the task of finding the document from the documents in some collection that best matches a query. For IR we also represent a query by a vector, also of length \(\mid\mathcal{V}\mid\), and we will need a way to compare two vectors to find how similar they are.</p>

<p>Later we will introduce the vector comparison processes: the tf-idf term weighting, and the cosine similarity metric.</p>

<h3 id="words-as-vectors">Words as Vectors</h3>

<p>As discussed above, we can represent a word as the row vector in term-document matrix. Similar words will have similar vectors because they tend to occur in similar documents.</p>

<p>However, for word vector representation, rather than the term-document matrix, it is more common to use the <strong>word-word matrix</strong> (or term-term matrix, or term-context matrix). This matrix is of dimensionality \(\mid\mathcal{V}\mid \times \mid\mathcal{V}\mid\), and each cell records the number of times the row (target) word and the column (context) word co-occur in some context in some training corpus. The context could be a document, or a window around the word.</p>

<p>Figure 6.5 shows a simplified subset of the word-word co-occurrence matrix for these four words computed from the Wikipedia corpus.</p>

<p><img src="/pictures/word_word_co_occurrence_matrix.png" alt="image-20200622023815889" style="zoom:57%;" /></p>

<p>Note that \(\mid\mathcal{V}\mid\), the length of the word vector, is generally the size of the vocabulary, usually between \(10,000\) and \(50,000\) words (using the most frequent words in the training corpus; keeping words after about the most frequent \(50,000\) or so is generlly not helpful). These vectors are usually very <strong>sparse</strong> since most of these numbers are zero, and there are efficient algorithms for storing and computing with sparse matrices.</p>

<h3 id="cosine-for-measuring-similarity">Cosine for Measuring Similarity</h3>

<p>By far the most common similarity metric for two vectors is the cosine of the angle between the vectors. The cosine similarity is based on the <strong>dot product</strong> (or inner product) operator:</p>

\[\mathbf{a}^T \mathbf{b} = \sum_i a_ib_i.\]

<p>Most metrics for similarity between vectors are based on the dot product, because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, orthogonal vectors have a dot product of $0$, representing their strong dissimilarity.</p>

<p>This raw dot product, however, has a problem as a similarity metric: it favors long vectors. The <strong>vector length</strong> is defined as</p>

\[\| \mathbf{a} \|_2 = \sqrt{\sum_i a_i^2}.\]

<p>More frequent words have longer vectors, since they tend to co-occur with more words and have higher co-occurrence values with each of them. The raw dot product thus will be higher for frequent words. But this is a problem since we would like a similarity metric that tells us how similar two words are regardless of their frequency. The simplest way to modify the dot product to normalize for the vector length is to divide the dot product by the lengths of each of the two vectors.</p>

<p>This normalized dot product turns out to be the same as the cosine of the angle between the two vectors:</p>

\[\cos (\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a}^T \mathbf{b}}{\| \mathbf{a} \|_2 \| \mathbf{b} \|_2 } = \frac{\sum_i a_ib_i}{\sqrt{\sum_i a_i^2} \sqrt{\sum_i b_i^2}}.\]

<h2 id="tf-idf">TF-IDF</h2>

<p>The intuition is that, some words (like <em>the, it</em>) are so unspecific that they occur in many documents and co-occur with almost all words, thus they are less informative than others. The TF-IDF algorithm penalize the term frequency by its unspecificity (document frequency). The TF-IDF is the product of term frequency (TF) and inverse document frequency (IDF).</p>

<p><strong>Term Frequency</strong> (<strong>TF</strong>) measures how often does the term \(t\) occur in the document $d$. The importance does not increase linearly with the term frequency, so term frequency $\text{Count}(t)$ is often made smaller by log transformation, as 
\(\text{tf}(t,d) := \log\left(1+\text{Count}(t,d)\right),\)</p>

<p>where the logarithm base is usually \(10\).</p>

<p>Document frequency is \(\text{df}(t) := \frac{n(t)}{N}\), if term $t$ occurs in $n(t)$ out of $N$ documents. e.g. $\frac{1}{10}$ if “dog” occurs in only $1$ of $10$ documents. Then the <strong>Inverse Document Frequency</strong> (<strong>IDF</strong>) formula is defined as</p>

\[\text{idf}(t) := \log \frac{1}{\text{df}(t)} = \log \frac{N}{n(t)}.\]

<p>Higher of \(\text{idf}(t)\) means more specific of the term $t$.</p>

<p>Therefore, the TF-IDF is the product of TF and IDF:</p>

\[\text{tfidf}(t,d) = \text{tf}(t,d) \times \text{idf}(t) = \log (1+\text{Count}(t,d)) \times \log \frac{N}{n(t)}.\]

<p>Overall, TF is big when term $t$ occurs often in document, and IDF is big when term $t$ does not occur in many other documents.</p>

<p>Cosine similarity works better with TF-IDF values than with original frequencies.</p>

<p><br /></p>

<p><strong>Reference:</strong></p>

<p>Jurafsky, D., Martin, J. H. (2009). <em>Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition</em>. Upper Saddle River, N.J.: Pearson Prentice Hall.</p>

  </div>

  <br> </br>
  <p><font color="grey" size="4"> Comments </font></p>
  <HR color=#D1D0CE SIZE=10>

<div id="disqus_thread"></div>

<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://walkermao.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


                            

  
</div>

      </section>
    </main><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178951885-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178951885-1');
</script>

    
    <div id="back-top">
      <a href="javascript:void(0);" onclick="topFunction()" title="Back to top"> </a>
    </div>

  </body>
</html>

<script src = "/assets/js/scroll_into_view.js"></script>
<script src = "/assets/js/back_to_top.js"></script>