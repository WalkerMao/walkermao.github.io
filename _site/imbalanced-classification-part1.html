<!DOCTYPE html>
<html lang="en">
  <head>
    <script src="https://kit.fontawesome.com/123ecac47c.js" crossorigin="anonymous"></script><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="shortcut icon" type="image/png" href="/assets/portfolio.png">
<title>Imbalanced Classification (Part 1)</title>
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Imbalanced Classification (Part 1) | Weikai’s blog.</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Imbalanced Classification (Part 1)" />
<meta name="author" content="Weikai Mao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally. This problem can have a significant impact on the effectiveness of the model." />
<meta property="og:description" content="Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally. This problem can have a significant impact on the effectiveness of the model." />
<link rel="canonical" href="http://localhost:4000/imbalanced-classification-part1.html" />
<meta property="og:url" content="http://localhost:4000/imbalanced-classification-part1.html" />
<meta property="og:site_name" content="Weikai’s blog." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-24T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Imbalanced Classification (Part 1)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Weikai Mao"},"dateModified":"2020-07-24T00:00:00+08:00","datePublished":"2020-07-24T00:00:00+08:00","description":"Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally. This problem can have a significant impact on the effectiveness of the model.","headline":"Imbalanced Classification (Part 1)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/imbalanced-classification-part1.html"},"url":"http://localhost:4000/imbalanced-classification-part1.html"}</script>
<!-- End Jekyll SEO tag -->


<meta name="google-site-verification" content="wXp8C1QlYKCpKxfXyFfQXEv9l5fJvcOi53ofYmOcaSA" />
<meta name="msvalidate.01" content="97F0BB32D312B808156DE357EA8474D3" />
<meta name="yandex-verification" content="690106a82d8966ab" />

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ["$","$"], ["\\(","\\)"] ],
        displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno"
      }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>    


  







</head>
  <body>
    <main class="container">
      <section class="about">
        <a href="/" class="iconlink"> 
          <h3> 
            <i class="fas fa-home"></i> HOME 
          </h3> 
        </a>
        <a href="/"> <img src="/assets/portfolio.png" alt="Weikai Mao"></a>
        <h2 id="title"> Weikai Mao </h2>
        <p style="font-size:90%" class="tagline">maoweikai123@outlook.com</p>
        
        <ul class="social"><a href="https://github.com/WalkerMao">
              <li>
                <i class="fab fa-github"></i>
              </li>
            </a><a href="https://www.linkedin.com/in/weikai-mao-000249124">
              <li>
                <i class="fab fa-linkedin"></i>
              </li>
            </a><a href="/wechat-qr-code.html">
              <li>
                <i class="fab fa-weixin"></i>
              </li>
            </a></ul><h3 style="color:gray; font-weight:normal"> Categories: </h3>
            <ul class="post-categories" style="max-width:225px;">
              
                  <a class="post-link" href="/"> <li style="padding: 4px 8px;"> All </li> </a>
              
                  <a class="post-link" href="/categories/cs"> <li style="padding: 4px 8px;"> CS </li> </a>
              
                  <a class="post-link" href="/categories/cv"> <li style="padding: 4px 8px;"> CV </li> </a>
              
                  <a class="post-link" href="/categories/dl"> <li style="padding: 4px 8px;"> DL </li> </a>
              
                  <a class="post-link" href="/categories/fe"> <li style="padding: 4px 8px;"> FE </li> </a>
              
                  <a class="post-link" href="/categories/math"> <li style="padding: 4px 8px;"> Math </li> </a>
              
                  <a class="post-link" href="/categories/ml"> <li style="padding: 4px 8px;"> ML </li> </a>
              
                  <a class="post-link" href="/categories/nlp"> <li style="padding: 4px 8px;"> NLP </li> </a>
              
                  <a class="post-link" href="/categories/stat"> <li style="padding: 4px 8px;"> Stat </li> </a>
              
                  <a class="post-link" href="/categories/杂"> <li style="padding: 4px 8px;"> 杂 </li> </a>
              
            </ul><p>&copy; 2023 </p>

      </section>
      <section class="content">
        <div class="sidebar">
  <ul><li><a href="#the-effect-of-class-imbalance">The Effect of Class Imbalance</a></li><li><a href="#model-selection">Model Selection</a></li><li><a href="#changing-performance-metric">Changing Performance Metric</a><ul><li><a href="#f1-score">F1 Score</a></li><li><a href="#cohens-kappa-coefficient">Cohen’s kappa Coefficient</a></li></ul></li></ul></li><li><a href="#changing-cutoffs">Changing Cutoffs</a></li></ul>

</div><div class="post-container" id="viewpoint">
  <a class="post-link" href="/imbalanced-classification-part1.html">
    <h2 class="post-title">Imbalanced Classification (Part 1)</h2>
  </a>
  <div class="post-meta">
    <div>
      <ul class="post-categories"><a class="post-link" href="/categories/ml"> <li>ML</li> </a></ul>
      <ul class="post-tags"></ul>
    </div>
    <div class="post-date">
      <i class="icon-calendar"></i>
      Jul 24, 2020
    </div>
  </div>
  <div style="line-height:77%;">
    <br>
  </div>
  <div class="post"><p>Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally. This problem can have a significant impact on the effectiveness of the model.</p>

<h2 id="the-effect-of-class-imbalance">The Effect of Class Imbalance</h2>

<p>Let’s first review the confusion matrix, as shown below.</p>

<div style="text-align: center"> <img src="../../../pictures/ConfusionMatrx.jpg" alt="Confusion Matrix" style="zoom:95%;" /> </div>

<p>Say if we have a imbalanced data set, in which \(99\%\) are of negative class and \(1\%\) are of positive class, then a classification model that always predict “negative” can achieve \(0.99\) accuracy on average. The specificity (true negative rate) of this model is \(\text{TNR} = \frac{TN}{TN+FP} = \frac{99\%}{99\% + 0\%}=1\). However, the sensitivity (true positive rate, or recall) is \(\text{TPR} = \frac{TP}{TP+FN} = \frac{0\%}{0\%+1\%}=0\), which is the fraction of the total amount of positive instances that were actually retrieved.</p>

<p>In this case, the ROC curve is a straight line and the AUC is $0.5$, as shown below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">test</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">test</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">predict</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">predict</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'False Positive Rate'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True Positive Rate'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div style="text-align: center"> <img src="../../../pictures/Special-ROC.png" alt="Special-ROC" style="zoom:100%;" /> </div>

<p>In the following sections, we discuss the approaches to deal with the class imbalance problems.</p>

<h2 id="model-selection">Model Selection</h2>

<p>Some models like Logistic Regression with loss functions of the form \(\frac{1}{n}\sum_{i=1}^{n}\text{Loss}(\hat{y}_i, y_i)\) are very sensitive to imbalanced dataset. However, models like Decision Tree, Ensemble Trees, and SVM are less sensitive.</p>

<p>As for the tree based methods, the objective of splitting a node is to maximize the information gain and make the sub-nodes as pure as possible, so tree based methods also pay attention to minority class because the minority class can also make the node impure.</p>

<p>As for the SVM, the loss is composed by margin and support vectors, not by all data, thus SVM works well if the support vectors are balanced, and the balanced original dataset is not that necessary.</p>

<h2 id="changing-performance-metric">Changing Performance Metric</h2>

<p>Note that the metrics are not the loss functions.</p>

<p>Accuracy is not the metric to use when working with an imbalanced dataset, as we have seen that it is misleading. Instead, we can use <strong>F1 score</strong>, <strong>Cohen’s kappa coefficient</strong>, or <strong>AUC</strong>.</p>

<h4 id="f1-score">F1 Score</h4>

<p>The <strong>F1 score</strong> is the harmonic mean of the precision and recall (sensitivity), where an F1 score reaches its best value at $1$ (perfect precision and recall). It is defined as</p>

\[F_1 := \frac{2}{\frac{1}{\text{Recall}} + \frac{1}{\text{Precision}}} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{TP}{TP + \frac{1}{2}(FP+FN)}.\]

<h4 id="cohens-kappa-coefficient">Cohen’s kappa Coefficient</h4>

<p><strong>Cohen’s kappa coefficient</strong> measures the agreement between two raters. In classification problem, these two raters are true labels and predictions, and the Cohen’s kappa coefficient represents the classification accuracy normalized by the imbalance of the classes in the data. It basically tells you how much better your classifier is performing over the performance of a classifier that simply guesses at random according to the frequency of each class.</p>

<p>The probability of observed agreement is</p>

\[P_o := \frac{\text{Observed agreement}}{\text{Total}} = \frac{TP+TN}{TP+TN+FP+FN} = \text{Accuracy}.\]

<p>The probability of expected (random) agreement is</p>

\[P_e := \frac{TP+FN}{\text{Total}} \cdot \frac{TP+FP}{\text{Total}} + \frac{FP+TN}{\text{Total}} \cdot \frac{FN+TN}{\text{Total}}.\]

<p>The formula for Cohen’s kappa coefficient is the probability of agreement take away the probability of expected (random) agreement divided by $1$ minus the probability of random agreement:</p>

\[\kappa := \frac{P_o - P_e}{1-P_e} = 1 - \frac{1-P_o}{1-P_e}.\]

<p>Cohen’s kappa is always less than or equal to $1$. Values of $0$ or less, indicate that the classifier is useless. There is no standardized way to interpret its values. Landis and Koch (1977) provide a way to characterize values. According to their scheme a value $&lt; 0$ is indicating no agreement, $0\sim0.20$ as slight, \(0.21\sim0.40\) as fair, \(0.41\sim0.60\) as moderate, \(0.61\sim0.80\) as substantial, and \(0.81\sim1\) as almost perfect agreement.</p>

<h2 id="changing-cutoffs">Changing Cutoffs</h2>

<p>To increase the prediction accuracy of the minority class samples, we can <strong>determine alternative cutoffs (or thresholds) for the predicted probabilities</strong>. The most straightforward approach is to use the ROC curve since it calculates the sensitivity and specificity across a continuum of cutoffs. Using this curve, an appropriate balance between sensitivity and specificity can be determined.</p>

<p>The plot below is an ROC curve labeled with different probability cutoffs (or thresholds), and the numbers in the parentheses are the specificity and sensitivity.</p>

<div style="text-align: center"> <img src="../../../pictures/ROC-with-different-cutoffs.png" alt="ROC-with-different-cutoffs" style="zoom:40%;" />  </div>

<p>Several techniques exist for determining a new cutoff. First, if there is a particular target that must be met for the sensitivity or specificity, this point can be found on the ROC curve and the corresponding cutoff can be determined. Another approach for determining a cutoff is to find the point on the ROC curve that is closest (i.e. the shortest distance) to the upper left corner of the plot. Another approach is to find the cutoff associated with the largest value of the <strong>Youden’s $J$ index</strong>. The index is defined as</p>

\[J := \text{Sensitivity} + \text{Specificity} - 1.\]

<p>In the ROC curve above, we can see that the cutoff $0.064$ is the best with considering either the distance or Youden’s $J$ index.</p>

<p>Note that changing cutoff does not influence the model parameters, and thus it does not increase the overall predictive effectiveness of the model. The main impact that an alternative cutoff has is to make trade-offs between particular types of errors.</p>

<p><br /></p>

<p><strong>References:</strong></p>

<p>Kuhn, M., &amp; Johnson, K. (2013). Remedies for severe class imbalance. In <em>Applied predictive modeling</em> (pp. 419-443). Springer, New York, NY.</p>

<p>Kampakis, S. S. (2016, May 8). Performance Measures: Cohen’s Kappa statistic. <em>The data scientist</em>. Retrieved July 23, 2020, from https://thedatascientist.com/performance-measures-cohens-kappa-statistic.</p>

<p>Pykes, K. (2020, Feb 27). Cohen’s Kappa. <em>Toward data science</em>. Retrieved July 23, 2020, from https://towardsdatascience.com/cohens-kappa-9786ceceab58.</p>

<p>Landis, J. R., &amp; Koch, G. G. (1977). The measurement of observer agreement for categorical data. <em>biometrics</em>, 159-174.</p>

<p>Weiss, G., &amp; Provost, F. (2001). The Effect of Class Distribution on Classifier Learning: An Empirical Study. <em>Department of Computer Science</em>, Rutgers University.</p>

  </div>

  <br> </br>
  <p><font color="grey" size="4"> Comments </font></p>
  <HR color=#D1D0CE SIZE=10>

<div id="disqus_thread"></div>

<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://walkermao.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


                            

  
</div>

      </section>
    </main><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178951885-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178951885-1');
</script>

    
    <div id="back-top">
      <a href="javascript:void(0);" onclick="topFunction()" title="Back to top"> </a>
    </div>

  </body>
</html>

<script src = "/assets/js/scroll_into_view.js"></script>
<script src = "/assets/js/back_to_top.js"></script>