<!DOCTYPE html>
<html lang="en">
  <head>
    <script src="https://kit.fontawesome.com/123ecac47c.js" crossorigin="anonymous"></script><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="shortcut icon" type="image/png" href="/assets/portfolio.png">
<title>支持向量机（SVM）</title>
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>支持向量机（SVM） | Weikai’s blog.</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="支持向量机（SVM）" />
<meta name="author" content="Weikai Mao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Switch to English verison" />
<meta property="og:description" content="Switch to English verison" />
<link rel="canonical" href="http://localhost:4000/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.html" />
<meta property="og:url" content="http://localhost:4000/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.html" />
<meta property="og:site_name" content="Weikai’s blog." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-17T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="支持向量机（SVM）" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Weikai Mao"},"dateModified":"2020-12-17T00:00:00+08:00","datePublished":"2020-12-17T00:00:00+08:00","description":"Switch to English verison","headline":"支持向量机（SVM）","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.html"},"url":"http://localhost:4000/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.html"}</script>
<!-- End Jekyll SEO tag -->


<meta name="google-site-verification" content="wXp8C1QlYKCpKxfXyFfQXEv9l5fJvcOi53ofYmOcaSA" />
<meta name="msvalidate.01" content="97F0BB32D312B808156DE357EA8474D3" />
<meta name="yandex-verification" content="690106a82d8966ab" />

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ["$","$"], ["\\(","\\)"] ],
        displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno"
      }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>    


  







</head>
  <body>
    <main class="container">
      <section class="about">
        <a href="/" class="iconlink"> 
          <h3> 
            <i class="fas fa-home"></i> HOME 
          </h3> 
        </a>
        <a href="/"> <img src="/assets/portfolio.png" alt="Weikai Mao"></a>
        <h2 id="title"> Weikai Mao </h2>
        <p style="font-size:90%" class="tagline">maoweikai123@outlook.com</p>
        
        <ul class="social"><a href="https://github.com/WalkerMao">
              <li>
                <i class="fab fa-github"></i>
              </li>
            </a><a href="https://www.linkedin.com/in/weikai-mao-000249124">
              <li>
                <i class="fab fa-linkedin"></i>
              </li>
            </a><a href="/wechat-qr-code.html">
              <li>
                <i class="fab fa-weixin"></i>
              </li>
            </a></ul><h3 style="color:gray; font-weight:normal"> Categories: </h3>
            <ul class="post-categories" style="max-width:225px;">
              
                  <a class="post-link" href="/"> <li style="padding: 4px 8px;"> All </li> </a>
              
                  <a class="post-link" href="/categories/cs"> <li style="padding: 4px 8px;"> CS </li> </a>
              
                  <a class="post-link" href="/categories/cv"> <li style="padding: 4px 8px;"> CV </li> </a>
              
                  <a class="post-link" href="/categories/dl"> <li style="padding: 4px 8px;"> DL </li> </a>
              
                  <a class="post-link" href="/categories/fe"> <li style="padding: 4px 8px;"> FE </li> </a>
              
                  <a class="post-link" href="/categories/math"> <li style="padding: 4px 8px;"> Math </li> </a>
              
                  <a class="post-link" href="/categories/ml"> <li style="padding: 4px 8px;"> ML </li> </a>
              
                  <a class="post-link" href="/categories/nlp"> <li style="padding: 4px 8px;"> NLP </li> </a>
              
                  <a class="post-link" href="/categories/stat"> <li style="padding: 4px 8px;"> Stat </li> </a>
              
                  <a class="post-link" href="/categories/杂"> <li style="padding: 4px 8px;"> 杂 </li> </a>
              
            </ul><p>&copy; 2023 </p>

      </section>
      <section class="content">
        <div class="sidebar">
  <ul><li><a href="#引入">引入</a></li><li><a href="#软边缘支持向量机">软边缘支持向量机</a></li><li><a href="#对偶问题">对偶问题</a><ul><li><a href="#对偶优化问题">对偶优化问题</a></li><li><a href="#smo算法">SMO算法</a></li><li><a href="#原问题的解">原问题的解</a></li><li><a href="#支持向量">支持向量</a></li></ul></li><li><a href="#核svm">核SVM</a><ul><li><a href="#映射到高维空间">映射到高维空间</a></li><li><a href="#核技巧">核技巧</a></li></ul></li><li><a href="#其他">其他</a></li></ul>

</div><div class="post-container" id="viewpoint">
  <a class="post-link" href="/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.html">
    <h2 class="post-title">支持向量机（SVM）</h2>
  </a>
  <div class="post-meta">
    <div>
      <ul class="post-categories"><a class="post-link" href="/categories/ml"> <li>ML</li> </a></ul>
      <ul class="post-tags"><a class="post-link" href="/tags/ml models"> <li>ML models</li> </a></ul>
    </div>
    <div class="post-date">
      <i class="icon-calendar"></i>
      Dec 17, 2020
    </div>
  </div>
  <div style="line-height:77%;">
    <br>
  </div>
  <div class="post"><p style="font-size:90%; text-align:right"> <a href="/svm.html">Switch to English verison</a> </p>

<h2 id="引入">引入</h2>

<p>对于二元分类问题，一个基本的思想是在样本空间中找到一个超平面，将样本分为两类。对于任意给定的\(w\in\mathbb{R}^p，b\in\mathbb{R}\),空间\(\mathbb{R}^p\)中的超平面可以表示为</p>

\[\{x\in\mathbb{R}^p:w^Tx+b=0\}，\]

<p>我们将此超平面记作\((w,b)\)。</p>

<p>假设\(y\in\{-1,1\}\)中，如果超平面可以分离样本，那么</p>

\[\begin{cases}
w^Tx_i+b\geq 1, &amp;\text{if }y_i=1, \\
w^Tx_i+b\leq-1, &amp;\text{if }y_i=-1.
\end{cases}\]

<p>将两个<strong>超平面</strong>表示为</p>

\[\begin{aligned} &amp; H_1 = \{x \in \mathbb{R}^p : w^Tx + b = 1\}, \\ &amp; H_2 = \{x \in \mathbb{R}^p : w^Tx + b = -1\}. \end{aligned}\]

<p>\(H_1\)和\(H_2\)是平行的，因为\(H_1\cap H_2=\emptyset\)。我们希望这两个超平面将样本分开，并且超平面之间的间隔最大化。</p>

<p>\(H_1\)和\(H_2\)之间的<strong>间隔</strong>为\(\frac{2}{\|w\|}\)。证明：</p>

<p>我们首先证明，对于任意\(x_1\in\mathbb{R}^p\)，样本空间中的点\(x_1\)与超平面\((w,b)\)之间的距离为 \(\frac{\mid w^Tx_1+b \mid}{\| w \|}\)。</p>

<p>对于超平面\((w,b)\)上的任意两点\(x_2,x_3\)，\(x_2-x_3\)是超平面\((w,b)\)上的向量，则\(w^T(x_2-x_3)=0\)，这意味着向量\(w\)垂直于\((w,b)\)。将\(x_0\)表示为\(x_1\)在\((w,b)\)上的投影点，\(x_1\)与\((w,b)\)之间的距离为\(\| x_1-x_0\|\)，则向量\(x_1-x_0\)与\((w,b)\)垂直，那么我们可以将其表示为\(x_1-x_0=\| x_1-x_0\|\frac{w}{\|w\|}\)。因此距离\(\|x_1-x_0\| = \sqrt{(x_1 - x_0)^T(x_1-x_0)} = \sqrt{\|x_1-x_0\| \frac{w^T}{\|w\|} (x_1 - x_0)} = \sqrt{ \frac{\|x_1-x_0\|}{\|w\|} (w^Tx_1 - w^Tx_0)} = \sqrt{ \frac{\|x_1-x_0\|}{\|w\|} (w^Tx_1 + b)}\) \(\implies \|x_1-x_0\|=\frac{\mid w^Tx_1+b \mid}{\| w \|}\)。</p>

<p>现在我们假设\(x_1\)在超平面\(H_1\)上，那么\(x_1\)和\(H_2\)之间的距离是\(\frac{\mid w^Tx_1+b+1 \mid}{\| w \|}=\frac{2}{\| w \|}. \square\)</p>

<p>我们注意到，最大化\(\frac{2}{\| w \|}\)与最小化\(\frac{\|w\|}{2}\)等价。</p>

<p>那么，对于<strong>线性可分</strong>的数据，支持向量机最大化两个超平面\(H_1,H_2\)的间隔：</p>

\[\begin{equation*}
\begin{aligned}
&amp; \min_{w,b} \frac{\|w\|}{2}, \\
&amp; \text{ subject to } y_i(w^Tx_i+b) \geq 1, \forall i=1,\cdots,n.
\end{aligned}
\end{equation*}\]

<p>样本点\(\{(x_i, y_i): y_i(w^Tx_i+b) = 1\}\)位于超平面\(H_1,H_2\)上，称为<strong>支持向量</strong>。</p>

<h2 id="软边缘支持向量机">软边缘支持向量机</h2>

<p>实际数据通常不是线性可分的。设</p>

\[z_i=y_i(w^Tx_i+b)-1，\]

<p>那么错误分类的样本是\(\{(x_i，y_i)：z_i&lt;0\}\)。</p>

<p>我们可以用损失函数来衡量误分类的严重程度。如0/1损失：</p>

\[\ell_{\text{0/1}}(z)=\mathbf{1}(z&lt;0).\]

<p>然而，这种损失函数不是凸函数，也不是连续的，因此不适合数值优化。因此，我们可以使用一些凸的连续的损失函数，称为<strong>替代损失</strong>函数。以下是三种常用的替代损失函数：</p>

\[\begin{aligned}
&amp;\text{合叶损失 Hinge loss:}&amp;\ell_{\text{hinge}}(z)=\max(0，1-z);\\
&amp;\text{指数损失 Expometal loss:}&amp;\ell_{\text{exp}}(z)=e^{-z};\\
&amp;\text{对数损失 Logistic loss:}&amp;\ell_{\text{log}}(z)=\ln(1+e^{-z}).
\end{aligned}\]

<p>以下是这四种损失函数的图形：</p>

<div align="center"> <img src="../../../pictures/surrogate-loss-functions.png" alt="surrogate-loss-functions.png" style="zoom:38%;" /> </div>

<p>相较于其他两个替代损失函数，合叶损失对训练数据中的极端样本更鲁棒。本文以下内容我们使用合叶损失作为替代损失函数。</p>

<p>对每一个样本\((x_i,y_i)\)，我们引入<strong>松弛变量</strong>\(\xi_i\)</p>

\[\xi_i = \ell_{\text{hinge}}(z_i+1) = \max\left(0, 1 - y_i(w^Tx_i+b)\right).\]

<div align="center"> <img src="../../../pictures/svm-hinge-loss.png" alt="svm-hinge-loss" style="zoom:50%;" /> </div>

<p>对于给定的超参数 \(C &gt; 0\), 软间隔支持向量机的优化问题可表示为</p>

\[\begin{equation*}
\begin{aligned}
&amp; \min_{w,b} \frac{\|w\|}{2} + C \sum_{i=1}^{n} \xi_i,\\
&amp; \text{ subject to } 
\begin{cases} 
y_i(w^Tx_i+b) \geq 1-\xi_i,  \\ \xi_i\geq0, 
\end{cases}
\ \ \forall i=1,\cdots,n.
\end{aligned}
\end{equation*}\]

<p>样本点\(\{(x_i, y_i): y_i(\hat{w}^Tx_i+\hat{b}) \leq 1\}\)是支持向量。</p>

<p>设\(\hat{w},\hat{b}\)为以上优化问题的最优解，那么<strong>分隔超平面</strong>为</p>

\[\{x\in\mathbb{R}^p:\hat{w}^T x+\hat{b}=0\}\]

<p>分类决策函数为</p>

\[\hat{y}=\text{sign}(\hat{w}^Tx+\hat{b})。\]

<h2 id="对偶问题">对偶问题</h2>

<h3 id="对偶优化问题">对偶优化问题</h3>

<p>软间隔支持向量机的优化问题对应的<strong>拉格朗日函数</strong>为
\(\mathcal{L}(w,b,\xi,\alpha,\mu) = \frac{\|w\|^2}{2} + C \sum_{i=1}^{n} \xi_i + \sum_{i=1}^n \alpha_i [1 - \xi_i - y_i(w^Tx_i + b)] - \sum_{i=1}^n\mu_i\xi_i,\)
其中\(\alpha_i \geq 0, \mu_i \geq 0\)是拉格朗日乘子。</p>

<p>设
\(\mathcal{D}(\alpha,\mu) = \min_{w,b,\xi} \mathcal{L}(w,b,\xi,\alpha,\mu).\)
则软间隔支持向量机的优化问题对应的对偶优化问题为</p>

\[\max_{\alpha, \mu: \alpha_i \geq 0, \mu_i \geq 0} \mathcal{D}(\alpha,\mu) = \max_{\alpha, \mu: \alpha_i \geq 0, \mu_i \geq 0} \min_{w,b,\xi} \mathcal{L}(w,b,\xi,\alpha,\mu).\]

<p>将函数\(\mathcal{L}(w,b,\xi,\alpha,\mu)\)分别对\(w,b,\xi_i\)求导，然后设为\(0\), 得到</p>

\[\begin{aligned}
&amp; \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial w} = w - \sum_{i=1}^{n} \alpha_i y_i x_i = 0 \implies w = \sum_{i=1}^{n} \alpha_i y_i x_i, \\
&amp; \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial b} = - \sum_{i=1}^{n} \alpha_i y_i = 0 \implies \sum_{i=1}^{n} \alpha_i y_i = 0, \\
&amp; \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial \xi_i} = C - \alpha_i - \mu_i = 0 \implies \alpha_i + \mu_i = C.
\end{aligned}\]

<p>代入回函数\(\mathcal{L}(w,b,\xi,\alpha,\mu)\)，则<strong>对偶优化问题</strong>可表示为</p>

\[\begin{equation*}
\begin{aligned}
&amp; \max_{\alpha} \mathcal{D}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{i'=1}^n \alpha_i\alpha_{i'}y_iy_{i'}x_i^Tx_{i'}, \\
&amp; \text{ subject to } 
\begin{cases} 
\sum_{i=1}^n \alpha_iy_i=0,  \\ 
0\leq\alpha_i\leq C ,
\end{cases}
\ \ \forall i=1,\cdots,n.
\end{aligned}
\end{equation*}\]

<p><strong>KKT条件</strong>为</p>

\[\begin{cases}
\alpha_i \geq 0, \\
\mu_i \geq 0, \\
y_i(w^Tx_i+b) - 1 + \xi_i \geq 0, \\
\alpha_i[y_i(w^Tx_i+b) - 1 + \xi_i] = 0, \\
\xi_i \geq 0, \\
\mu_i\xi_i = 0.
\end{cases}\]

<p>转换成对偶问题后，约束条件被简化了，并且可以使用核技巧。</p>

<h3 id="smo算法">SMO算法</h3>

<p>我们用<strong>SMO</strong>（序列最小最优化，sequential minimal optimization）算法来解该对偶优化问题。其基本思路是：如果所有变量的解满足KKT条件，那么这些变量就是最优化问题的解，因为KKT条件是该最优化问题的充分必要条件。</p>

<p>迭代以下两步直至收敛 {</p>

<ol>
  <li>找到一个违反KKT条件最严重的\(α_i\)，然后找到使得\(w^Tx_{i'}+b\)与真实值\(y_{i'}\)相差最大的\(i'\)对应的\(α_{i'}\)，这里的\(w=\sum_{l=1}^n \alpha_{l} y_{l}x_{l}\).</li>
  <li>维持其他的\(α_{i''}\)不变，在对偶优化问题的约束条件下求解二次规划问题：\(\max_{\alpha_i,\alpha_{i'}}\mathcal{D}(\alpha)\).
}</li>
</ol>

<p>该对偶优化问题的解为</p>

\[\hat{\alpha} = (\hat{\alpha}_1, \hat{\alpha}_2, \cdots, \hat{\alpha}_n) = \arg\max_{\alpha} \mathcal{D}(\alpha),\]

<p>SMO算法的详细介绍可参见李航的《统计学习方法》7.4 序列最小优化算法。</p>

<h3 id="原问题的解">原问题的解</h3>

<p>在求得解\(\hat{\alpha}\)后, 可计算</p>

\[\hat{w} = \sum_{i=1}^n \hat{\alpha}_{i} y_{i}x_{i}.\]

<p>下标在以下集合\(I^*\)中的样本点\((x_i,y_i)\)是支持向量：</p>

\[I^* = \{ i^*: \hat{\alpha}_{i^*} \neq 0 \} = \{ i^*: 0 &lt; \hat{\alpha}_{i^*} \leq C \}.\]

<p>然后我们可通过任一支持向量\((x_{i^*},y_{i^*})\)来计算截距项\(\hat{b}\)：</p>

\[\hat{b} = y_{i^*} - \hat{w}^Tx_{i^*} = y_{i^*} - \sum_{i=1}^n \hat{\alpha}_{i} y_{i}x_{i}^T x_{i^*}.\]

<p>在实践中，出于鲁棒性的考虑，我们通常采用所有（或部分）支持向量计算的\(\hat{b}\)的平均值:</p>

\[\hat{b} = \frac{1}{\mid I^* \mid} \sum_{i^* \in I^*} \left( y_{i^*} - \hat{w}^Tx_{i^*} \right) = \frac{1}{\mid I^* \mid} \sum_{i^* \in I^*} \left( y_{i^*} - \sum_{i=1}^n \hat{\alpha}_{i} y_{i}x_{i}^T x_{i^*} \right).\]

<p>在得到\(\hat{w}\)和\(\hat{b}\)后，分隔超平面为</p>

\[\left\{ x\in\mathbb{R}^p: \hat{w}^T{x} + \hat{b} = 0 \right\} = \left\{ x\in\mathbb{R}^p: \sum_{i=1}^n \hat{\alpha}_{i} y_{i}x_{i}^T x +  \frac{1}{\mid I^* \mid} \sum_{i^* \in I^*} \left( y_{i^*} - \sum_{i=1}^n \hat{\alpha}_{i} y_{i}x_{i}^T x_{i^*} \right) = 0 \right\}.\]

<p>分类决策函数为</p>

\[\hat{y} = \text{sign} \left( \hat{w}^Tx + \hat{b} \right).\]

<h3 id="支持向量">支持向量</h3>

<p>对于任一样本\((x_i,y_i)\), 有以下五种可能的情形。</p>

<p>若\(\alpha_i=0\), 则\(\mu_i=C，\xi_i=0\)。该样本（如下图中的\(x_1\)）不影响超平面\(\left\{ x\in\mathbb{R}^p: \hat{w}^T{x} + \hat{b} = 0 \right\}\)的估计，该样本也不是支持向量。</p>

<p>否则，若\(0 &lt; \alpha_i \leq C\)，则该样本为支持向量。</p>

<p>若\(0&lt;\alpha_i&lt;C\)，则\(\mu_i=C-\alpha_i，\xi_i=0\)。该样本（如下图中的\(x_2\)）被正确分类，且该样本在超平面\(H_1\)（若\(y_i=1\)）或\(H_2\)（若\(y_i=-1\)）上。</p>

<p>若\(\alpha_i=C\)，则\(\mu_i=0\)，而\(\frac{\xi_i}{\|w\|}\)为样本至超平面\(H_1\)（若\(y_i=1\)）或\(H_2\)（若\(y_i=-1\)）的距离。</p>

<p>若\(\alpha_i=C\)且\(0&lt;\xi_i&lt;1\)，则\(\mu_i=0\)。该样本（如下图中的\(x_3\)）被正确分类。</p>

<p>若\(\alpha_i=C\)且\(\xi_i=1\)，则\(\mu_i=0\)。该样本（如下图中的\(x_4\)）在分隔平面上。</p>

<p>若\(\alpha_i=C\)且 \(\xi_i&gt;1\)，则\(\mu_i=0\)。该样本（如下图中的\(x_5,x_6\)）被误分类。</p>

<p><br /></p>

<div align="center"> <img src="../../../pictures/support-vectors.png" alt="img" style="zoom:80%;" /> </div>

<p><br /></p>

<p>这五种情形总结为表格</p>

<table>
  <thead>
    <tr>
      <th>\(\alpha_i\)</th>
      <th>\(\mu_i\)</th>
      <th>\(\xi_i\)</th>
      <th>是否正确分类？</th>
      <th>是否为支持向量？</th>
      <th>是否在超平面上？</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\(0\)</td>
      <td>\(C\)</td>
      <td>\(0\)</td>
      <td>是</td>
      <td>否</td>
      <td>否</td>
    </tr>
    <tr>
      <td>\(0&lt;\alpha_i&lt;C\)</td>
      <td>\(C-\alpha_i\)</td>
      <td>\(0\)</td>
      <td>是</td>
      <td>是</td>
      <td>在超平面\(H_1\)（若\(y_i=1\)）或\(H_2\)（若\(y_i=-1\)）上</td>
    </tr>
    <tr>
      <td>\(C\)</td>
      <td>\(0\)</td>
      <td>\(0&lt;\xi_i&lt;1\)</td>
      <td>是</td>
      <td>是</td>
      <td>否</td>
    </tr>
    <tr>
      <td>\(C\)</td>
      <td>\(0\)</td>
      <td>\(1\)</td>
      <td>/</td>
      <td>是</td>
      <td>在分隔平面上</td>
    </tr>
    <tr>
      <td>\(C\)</td>
      <td>\(0\)</td>
      <td>\(\xi_i&gt;1\)</td>
      <td>否</td>
      <td>是</td>
      <td>若\(\xi_i=2\)，在超平面\(H_2\)（若\(y_i=1\)）或\(H_1\)（若\(y_i=-1\)）上；否则不在超平面上</td>
    </tr>
  </tbody>
</table>

<h2 id="核svm">核SVM</h2>

<h3 id="映射到高维空间">映射到高维空间</h3>

<p>如果原数据线性不可分，映射到高维空间后可以使之线性可分。</p>

<p>下图中，左边的数据点在原始特征空间中是线性不可分的，但在右边的更高维的特征空间中是线性可分的。</p>

<div align="center"> <img src="../../../pictures/Kernel-SVM.png" alt="Kernel-SVM" style="zoom:70%;" /> </div>

<p>设\(\phi:\mathbb{R}^p \to \mathbb{R}^\tilde{p}\)为低维到高维的映射函数，\(\phi(x)\)为将原特征向量\(x\)映射到高维空间之后的高维特征向量。
\(x \in \mathbb{R}^p \to \phi(x) \in \mathbb{R}^\tilde{p}.\)</p>

<p>在高维空间中的两个超平面为</p>

\[\{x \in \mathbb{R}^p : w^T \phi(x) + b = \pm 1\}.\]

<p>软间隔支持向量机的优化问题可表示为</p>

\[\begin{equation*}
\begin{aligned}
&amp; \min_{w,b} \frac{\|w\|}{2} + C \sum_{i=1}^{n} \xi_i, \\
&amp; \text{ subject to } 
\begin{cases} 
 y_i(w^T \phi(x_i)+b) \geq 1-\xi_i,  \\ 
 \xi_i \geq 0, 
\end{cases}
\ \ \forall i=1,\cdots,n.
\end{aligned}
\end{equation*}\]

<p>转化成对偶问题</p>

\[\begin{equation*}\begin{aligned}&amp; \min_{\alpha} \mathcal{D}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{i'=1}^n \alpha_i\alpha_{i'}y_iy_{i'} \phi(x_i)^T \phi(x_{i'}), \\
&amp; \text{ subject to } \begin{cases} \sum_{i=1}^n \alpha_iy_i=0,  \\ 
0\leq\alpha_i\leq C , \end{cases} \ \ \forall i=1,\cdots,n. \end{aligned} \end{equation*}\]

<h3 id="核技巧">核技巧</h3>

<p>向量\(\phi(x)\) 通常有较高的维度 \(\tilde{p}\), 计算点积 \(\phi(x_i)^T \phi(x_{i'})\) 的复杂度 \(O(\tilde{p})\) 通常较大. 为处理这一问题，我们采用<strong>核技巧</strong>。</p>

<p><strong>核函数</strong>定义为</p>

\[\mathcal{K}(x_i, x_{i'}) =  \phi(x_i)^T \phi(x_{i'}).\]

<p>我们可以使用核函数在原始特征空间\(\mathbb{R}^p\)中计算\(\mathcal{K}(x_i, x_{i'})\)来得到高维特征空间\(\mathbb{R}^\tilde{p}\)中的点积\(\phi(x_i)^T \phi(x_{i'})\)，而计算\(\mathcal{K}(x_i, x_{i'})\)的复杂度为\(O(p)\)，这远小于直接计算 \(\phi(x_i)^T \phi(x_{i'})\) 的复杂度 \(O(\tilde{p})\)。</p>

<p>使用核技巧之后，对偶问题转化为</p>

\[\begin{equation*}\begin{aligned}&amp; \min_{\alpha} \mathcal{D}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{i'=1}^n \alpha_i\alpha_{i'}y_iy_{i'} \mathcal{K}(x_i, x_{i'}), \\
&amp; \text{ subject to } \begin{cases} \sum_{i=1}^n \alpha_iy_i=0,  \\ 
0\leq\alpha_i\leq C , \end{cases} \ \ \forall i=1,\cdots,n. \end{aligned} \end{equation*}\]

<p>分隔超平面转化为</p>

\[\left\{ x\in\mathbb{R}^p: \hat{w}^T{x} + \hat{b} = 0 \right\} = \left\{ x\in\mathbb{R}^p: \sum_{i=1}^n \hat{\alpha}_{i} y_{i}\mathcal{K}(x_i, x) +  \frac{1}{\mid I^* \mid} \sum_{i^* \in I^*} \left( y_{i^*} - \sum_{i=1}^n \hat{\alpha}_{i} y_{i}\mathcal{K}(x_i, x_{i^*}) \right) = 0 \right\}.\]

<p>另外，<strong>核矩阵</strong>定义为</p>

\[\mathbf{K} = 
\begin{pmatrix}
  \mathcal{K}(x_1, x_1) &amp; \mathcal{K}(x_1, x_2) &amp; \cdots &amp; \mathcal{K}(x_1, x_{n}) \\
\mathcal{K}(x_2, x_1) &amp; \mathcal{K}(x_2, x_2) &amp; \cdots &amp; \mathcal{K}(x_2, x_{n}) \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  \mathcal{K}(x_n, x_1) &amp; \mathcal{K}(x_n, x_2) &amp; \cdots &amp; \mathcal{K}(x_n, x_{n})
 \end{pmatrix}.\]

<p>核矩阵是半正定的。其实，一个函数可以用作核函数的充分必要条件是，该函数是对称的且对应的核矩阵是半正定的。该函数是对称的是指对任意 \(i,i'\)都有\(\mathcal{K}(x_i, x_{i'}) = \mathcal{K}(x_{i'}, x_i)\)。</p>

<p>一些常用的核函数：</p>

<table>
  <thead>
    <tr>
      <th>核函数名称</th>
      <th>表达式</th>
      <th>参数</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>线性 Linear</td>
      <td>\(\mathcal{K}(x_i, x_{i'})=x_i^Tx_{i'}\)</td>
      <td> </td>
    </tr>
    <tr>
      <td>多项式 Polynomial</td>
      <td>\(\mathcal{K}(x_i, x_{i'})=(x_i^Tx_{i'})^d\)</td>
      <td>多项式次数 \(d &gt; 1\)</td>
    </tr>
    <tr>
      <td>RBF (or 高斯 Gaussian)</td>
      <td>\(\mathcal{K}(x_i, x_{i'})=\exp (-| x_i - x_{i'} |/2\sigma^2)\)</td>
      <td>宽度 \(\sigma&gt;0\)</td>
    </tr>
    <tr>
      <td>拉普拉斯 Laplace</td>
      <td>\(\mathcal{K}(x_i, x_{i'})=\exp (-| x_i - x_{i'}|/\sigma)\)</td>
      <td>\(\sigma&gt;0\)</td>
    </tr>
    <tr>
      <td>Sigmoid</td>
      <td>\(\mathcal{K}(x_i, x_{i'})=\text{tanh}(\beta x_i^Tx_{i'} + \theta)\)</td>
      <td>\(\beta&gt;0, \theta&lt;0\)</td>
    </tr>
  </tbody>
</table>

<p>选择核函数的经验准则：</p>

<ol>
  <li>如果原数据样本量不大，且特征维度高，通常这样的数据是线性（近似）可分的，可采用线性核函数；</li>
  <li>如果原数据样本量较大，且特征维度不高，通常使用高斯核函数。</li>
</ol>

<p>我们注意到，支持向量机若使用线性核函数则为线性分类器，否则为非线性核函数。</p>

<h2 id="其他">其他</h2>

<p>支持向量机的优化问题是凸优化，因此其解一定是全剧最优解。</p>

<p>由于支持向量机的构建只依赖于支持向量，相较于其他算法, 支持向量机更适合小样本高维数据的的建模。</p>

<p>支持向量机的构建对样本不均衡问题不太敏感，因为其优化目标极小化超平面间隔和支持向量带来的松弛变量。所以只要支持向量相对均衡，支持向量机受样本不均衡问题的影响就不会太大。</p>

<p><br /></p>

<p><strong>参考资料：</strong></p>

<p>Lecture notes from my professor <a href="https://sites.google.com/view/minxu/home">Min Xu</a>.</p>

<p>周志华. 第6章 支持向量机. <em>机器学习</em>. 清华大学出版社, 2016.</p>

<p>李航. 第7章 支持向量机. <em>统计学习方法</em>. 清华大学出版社, 2012.</p>

<p>张皓. <a href="https://link.zhihu.com/?target=https%3A//github.com/HaoMood/File/raw/master/%25E4%25BB%258E%25E9%259B%25B6%25E6%259E%2584%25E5%25BB%25BA%25E6%2594%25AF%25E6%258C%2581%25E5%2590%2591%25E9%2587%258F%25E6%259C%25BA%28SVM%29.pdf">从零推导支持向量机(SVM)</a>.</p>

  </div>

  <br> </br>
  <p><font color="grey" size="4"> Comments </font></p>
  <HR color=#D1D0CE SIZE=10>

<div id="disqus_thread"></div>

<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://walkermao.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


                            

  
</div>

      </section>
    </main><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178951885-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178951885-1');
</script>

    
    <div id="back-top">
      <a href="javascript:void(0);" onclick="topFunction()" title="Back to top"> </a>
    </div>

  </body>
</html>

<script src = "/assets/js/scroll_into_view.js"></script>
<script src = "/assets/js/back_to_top.js"></script>