<!DOCTYPE html>
<html lang="en">
  <head>
    <script src="https://kit.fontawesome.com/123ecac47c.js" crossorigin="anonymous"></script><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="shortcut icon" type="image/png" href="/assets/portfolio.png">
<title>Bagging and Random Forests</title>
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Bagging and Random Forests | Weikai’s blog.</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Bagging and Random Forests" />
<meta name="author" content="Weikai Mao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="1. Bootstrap" />
<meta property="og:description" content="1. Bootstrap" />
<link rel="canonical" href="http://localhost:4000/bagging-and-random-forests.html" />
<meta property="og:url" content="http://localhost:4000/bagging-and-random-forests.html" />
<meta property="og:site_name" content="Weikai’s blog." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-12T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Bagging and Random Forests" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Weikai Mao"},"dateModified":"2020-03-12T00:00:00+08:00","datePublished":"2020-03-12T00:00:00+08:00","description":"1. Bootstrap","headline":"Bagging and Random Forests","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/bagging-and-random-forests.html"},"url":"http://localhost:4000/bagging-and-random-forests.html"}</script>
<!-- End Jekyll SEO tag -->


<meta name="google-site-verification" content="wXp8C1QlYKCpKxfXyFfQXEv9l5fJvcOi53ofYmOcaSA" />
<meta name="msvalidate.01" content="97F0BB32D312B808156DE357EA8474D3" />
<meta name="yandex-verification" content="690106a82d8966ab" />

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ["$","$"], ["\\(","\\)"] ],
        displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno"
      }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>    


  







</head>
  <body>
    <main class="container">
      <section class="about">
        <a href="/" class="iconlink"> 
          <h3> 
            <i class="fas fa-home"></i> HOME 
          </h3> 
        </a>
        <a href="/"> <img src="/assets/portfolio.png" alt="Weikai Mao"></a>
        <h2 id="title"> Weikai Mao </h2>
        <p style="font-size:90%" class="tagline">maoweikai123@outlook.com</p>
        
        <ul class="social"><a href="https://github.com/WalkerMao">
              <li>
                <i class="fab fa-github"></i>
              </li>
            </a><a href="https://www.linkedin.com/in/weikai-mao-000249124">
              <li>
                <i class="fab fa-linkedin"></i>
              </li>
            </a><a href="/wechat-qr-code.html">
              <li>
                <i class="fab fa-weixin"></i>
              </li>
            </a></ul><h3 style="color:gray; font-weight:normal"> Categories: </h3>
            <ul class="post-categories" style="max-width:225px;">
              
                  <a class="post-link" href="/"> <li style="padding: 4px 8px;"> All </li> </a>
              
                  <a class="post-link" href="/categories/cs"> <li style="padding: 4px 8px;"> CS </li> </a>
              
                  <a class="post-link" href="/categories/cv"> <li style="padding: 4px 8px;"> CV </li> </a>
              
                  <a class="post-link" href="/categories/dl"> <li style="padding: 4px 8px;"> DL </li> </a>
              
                  <a class="post-link" href="/categories/fe"> <li style="padding: 4px 8px;"> FE </li> </a>
              
                  <a class="post-link" href="/categories/math"> <li style="padding: 4px 8px;"> Math </li> </a>
              
                  <a class="post-link" href="/categories/ml"> <li style="padding: 4px 8px;"> ML </li> </a>
              
                  <a class="post-link" href="/categories/nlp"> <li style="padding: 4px 8px;"> NLP </li> </a>
              
                  <a class="post-link" href="/categories/stat"> <li style="padding: 4px 8px;"> Stat </li> </a>
              
                  <a class="post-link" href="/categories/杂"> <li style="padding: 4px 8px;"> 杂 </li> </a>
              
            </ul><p>&copy; 2023 </p>

      </section>
      <section class="content">
        <div class="sidebar">
  <ul><li><a href="#1-bootstrap">1. Bootstrap</a></li><li><a href="#2-bagging">2. Bagging</a><ul><li><a href="#21-out-of-bag-error">2.1 Out of Bag Error</a></li><li><a href="#22-comparison-with-boosting">2.2 Comparison with Boosting</a></li></ul></li><li><a href="#3-random-forests">3. Random Forests</a><ul><li><a href="#31-reduce-the-correlation">3.1 Reduce the Correlation</a></li><li><a href="#32-algorithm">3.2 Algorithm</a></li><li><a href="#33-variable-importance">3.3 Variable Importance</a><ul><li><a href="#331-mean-decrease-in-impurity-mdi">3.3.1 Mean Decrease in Impurity (MDI)</a></li><li><a href="#332-mean-decrease-in-accuracy-mda">3.3.2 Mean Decrease in Accuracy (MDA)</a></li></ul></li><li><a href="#34-robustness-and-no-overfitting">3.4 Robustness and No Overfitting</a></li><li><a href="#tips">Tips</a></li></ul></li></ul>

</div><div class="post-container" id="viewpoint">
  <a class="post-link" href="/bagging-and-random-forests.html">
    <h2 class="post-title">Bagging and Random Forests</h2>
  </a>
  <div class="post-meta">
    <div>
      <ul class="post-categories"><a class="post-link" href="/categories/ml"> <li>ML</li> </a></ul>
      <ul class="post-tags"><a class="post-link" href="/tags/ensemble learning"> <li>Ensemble learning</li> </a><a class="post-link" href="/tags/ml models"> <li>ML models</li> </a></ul>
    </div>
    <div class="post-date">
      <i class="icon-calendar"></i>
      Mar 12, 2020
    </div>
  </div>
  <div style="line-height:77%;">
    <br>
  </div>
  <div class="post"><h2 id="1-bootstrap">1. Bootstrap</h2>

<p>The basic idea is to randomly draw datasets with replacement from the training data, each
sample the same size as the original training set. This is done \(B\) times (\(B = 100\) say), producing \(B\) bootstrap datasets.</p>

<p>Nonparametric bootstrap: sample with replacement from the training data. Parametric bootstrap: in which we simulate new responses by adding Gaussian noise to the predicted values.</p>

<p>Bootstrap can access the accuracy (e.g. derive estimates of standard errors and confidence intervals) of a parameter estimate or a prediction.</p>

<h2 id="2-bagging">2. Bagging</h2>

<p>Bootstrap aggregation or bagging averages the prediction over a collection of bootstrap samples, thereby reducing its variance. Bagging can dramatically reduce the variance of unstable procedures like trees, leading to improved prediction. For regression tree, we simply fit the same regression tree many times to bootstrap-sampled versions of the training data, and average the result. For classification, a committee of trees each cast a vote for the predicted class.</p>

<p>For each bootstrap sample $Z^{∗b},\ b = 1,2,…,B$, we fit our model, giving prediction $\hat{f}^{*b}(x)$. The bagging estimate is defined by</p>

\[\hat{f}_{\text{bag}}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{f}^{*b}(x).\]

<p>The bagged estimate $\hat{f}_{\text{bag}}(x)$ will differ from the original estimate $\hat{f}(x)$ only when the latter is a nonlinear or adaptive function of the data.</p>

<h3 id="21-out-of-bag-error">2.1 Out of Bag Error</h3>

<p>The probability of a observation is not be selected in the bootstrap sample is  $(1-\frac{1}{n})^n \to \frac{1}{e}$. For each bootstrap sample, we have about $36.8\%$ ($\frac{1}{e}$) of observations are not included in it. We can use these <strong>out-of-bag samples</strong> to compute the <strong>out-of-bag error</strong> (global estimate of the mean prediction error) of the model build on the bootstrap sample.</p>

<p>How to compute out-of-bag error?</p>

<p>For each $x_i$ in training data set, get prediction \(\hat{y}_{i \text{(oob)}}\) by the estimators (e.g. trees) that were trained without \(x_i\). Then the out-of-bag error is \(\frac{1}{N}\sum_{i=1}^{N} \text{Loss}(y_i, \hat{y}_{i\text{(oob)}})\).</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>$Z^{∗1} \to f^{*1}$</th>
      <th>$Z^{∗2} \to f^{*2}$</th>
      <th>$Z^{∗3} \to f^{*3}$</th>
      <th>…</th>
      <th>$Z^{∗B} \to f^{*B}$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$(x_1,y_1)$</td>
      <td>✓</td>
      <td>✗</td>
      <td>✗</td>
      <td>…</td>
      <td>✓</td>
    </tr>
    <tr>
      <td>$(x_2,y_2)$</td>
      <td>✗</td>
      <td>✗</td>
      <td>✓</td>
      <td>…</td>
      <td>✓</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
    </tr>
    <tr>
      <td>$(x_N,y_N)$</td>
      <td>✓</td>
      <td>✓</td>
      <td>✗</td>
      <td>…</td>
      <td>✗</td>
    </tr>
  </tbody>
</table>

<p>In the example above, $(x_2,y_2)$ is not used in bootstrap samples $Z^{∗1}$ and $Z^{∗2}$ to train the estimator \(f^{*1}\) and \(f^{*2}\), so \(\hat{y}_{2\text{(oob)}} = \text{average}\Big(\hat{f}^{*1}(x_2),\hat{f}^{*2}(x_2)\Big)\).</p>

<p>The OOB error estimate is almost identical to that obtained by cross-validation. Once the OOB error stabilizes, the training can be terminated.</p>

<h3 id="22-comparison-with-boosting">2.2 Comparison with Boosting</h3>

<p>Bagging can rarely get lower bias compared to a single model, however, boosting can. That is because each estimator $f^{*b}$ generated in bagging is identically distributed (i.d.), the expectation of an average of $B$ such estimators is the same as the expectation of any one of them. This means the bias of bagged estimators is the same as that of the individual estimator. This is in contrast to boosting, where the estimators are trained in an adaptive way to remove bias, and hence are not i.d.</p>

<p>Bagging can help to avoid over-fitting, but boosting can increase this problem.</p>

<table>
  <thead>
    <tr>
      <th><strong>Similarities</strong></th>
      <th><strong>Differences</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Both make the final decision by averaging some learners …</td>
      <td>… but it is an equally weighted average for Bagging and a weighted average for Boosting, more weight to those with better performance on training data.</td>
    </tr>
    <tr>
      <td>Both are good at reducing variance and provide higher stability…</td>
      <td>… but only Boosting tries to reduce bias. On the other hand, Bagging may solve the over-fitting problem, while Boosting can increase it.</td>
    </tr>
  </tbody>
</table>

<h2 id="3-random-forests">3. Random Forests</h2>

<p>The essential idea in bagging is to average many noisy but approximately unbiased models, and hence reduce the variance.  Bagging seems to work especially well for high-variance, low-bias procedures, such as trees. Trees are ideal candidates for bagging, since they can capture complex interaction structures in the data, and if grown sufficiently deep, have relatively low bias. Since trees are notoriously noisy (unstable), they benefit greatly from the averaging.</p>

<p>Random forests (Breiman, 2001) is a substantial modification of bagging that builds a large collection of de-correlated trees, and then averages them. On many problems the performance of random forests is very similar to boosting, and they are simpler to train and tune. As a consequence, random forests are popular.</p>

<h3 id="31-reduce-the-correlation">3.1 Reduce the Correlation</h3>

<p>Suppose there are $B$ i.i.d. random variables $t_1, \dots, t_B$, each with the variance $\sigma^2$. The average has variance $\frac{σ^2}{B}$. If the variables $t_1, \cdots , t_B$ are simply i.d. (identically distributed, but not necessarily independent) with positive pairwise correlation $ρ$, the variance of the average is</p>

\[\begin{align}
\text{Var}(\frac{1}{B} \sum_{b=1}^{B} t_b) &amp;= \frac{1}{B^2} \text{Var}( \sum_{b=1}^{B} t_b) \\
&amp;= \frac{1}{B^2} \sum_{b,b'=1}^{B} \text{Cov}(t_b, t_{b'}) \\
&amp;= \frac{1}{B^2} \Big[ \sum_{b=1}^{B} \text{Var}(t_b) + \sum_{b \neq b'} \text{Cov}(t_b, t_{b'}) \Big] \\
&amp;= \frac{1}{B^2} \Big[ B \sigma^2 + B(B-1) \rho \sigma^2 \Big] \\
&amp;= \rho \sigma^2 + \frac{1-\rho}{B} \sigma^2.
\end{align}\]

<p>As $B$ increases, $\rho \sigma^2 + \frac{1-\rho}{B} \sigma^2 \to \rho \sigma^2$, and hence the correlation $ρ$ of pairs of bagged trees limits the benefits of averaging.</p>

<p>The idea in random forests is to improve the variance reduction of bagging by reducing the correlation $\rho$ between the trees, without increasing the variance $\sigma^2$ too much. This is achieved in the tree-growing process through <strong>random selection of the input variables</strong>.</p>

<h3 id="32-algorithm">3.2 Algorithm</h3>

<p><strong>Algorithm: Random Forest for Regression or Classification.</strong></p>

<p>[1] For $b = 1$ to $B$, execute the steps (a) and (b):</p>

<p>   (a) Draw a bootstrap sample $Z^{∗b}$ of size $N$ from the training data.</p>

<p>   (b) Grow a random-forest tree $T_b$ to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until the minimum node size is reached.</p>

<p>     i. Select $m$ features at random from the $p$ features.</p>

<p>     ii. Pick the best variable/split-point among the $m$ features.</p>

<p>     iii. Split the node into two daughter nodes.</p>

<p>[2] Output the ensemble of trees \(\{ T_b \} ^B_1\).</p>

<h3 id="33-variable-importance">3.3 Variable Importance</h3>

<p>There are two popular metrics to measure variable importance.</p>

<h4 id="331-mean-decrease-in-impurity-mdi">3.3.1 Mean Decrease in Impurity (MDI)</h4>

<p>This is the same as the relative importance of predictor variables that is introduced in the section 10.13.1 of ESL.</p>

<p>For a single decision tree $T_b$, the importance of the variable \(X_j\) (denote as \(\mathcal{I}_j^2(T_b)\)) can be measured by the sum of the improvements in loss after partition regions by variable $X_j$ over all nodes.</p>

<p>For additive tree expansions, it is simply averaged over the trees \(\mathcal{I}_j^2 = \frac{1}{B} \sum_{b=1}^{B} \mathcal{I}_j^2(T_b)\), it can also be written as</p>

\[\hat{\text{MDI}}(X_j) = \frac{1}{B} \sum_{b=1}^{B} \sum_{\text{nonterminal} \\ \text{nodes}}(\text{Decrease in Impurity from spliting the j-th variable}).\]

<h4 id="332-mean-decrease-in-accuracy-mda">3.3.2 Mean Decrease in Accuracy (MDA)</h4>

<p>After we computed the OOB error, we randomly permute the values for the $j$-th variable ($j=1,…,p$), and fit the model with it and all other variables, and then compute the OOB error again. Permutation on the important variable leads to a large increase of OOB error.</p>

\[\hat{\text{MDA}}(X_j)= \text{Difference in the OOB error before and after randomly} \\ \text{permuting the values of }j\text{-th variable in the out-of-bag samples.}\]

<p>This metric is more computationally expensive and accurate.</p>

<h3 id="34-robustness-and-no-overfitting">3.4 Robustness and No Overfitting</h3>

<p>When the number of variables is large, but the fraction of relevant variables (useful variables) small, random forests are likely to perform poorly with small $m$.</p>

<p>Random Forests is <strong>robust</strong> with the appropriate $m$. When the number of relevant variables (useful variables) increases, the performance of random forests is surprisingly robust to an increase in the number of noise variables. For example, with $6$ relevant and $100$ noise variables, the probability of a relevant variable being selected at any split is $0.46$, assuming $m=\sqrt{6+100} \approx 10$.</p>

<p>Random forests “<strong>cannot overfit</strong>” the data. It is certainly true that increasing $B$ does not cause the random forest sequence to overfit.</p>

<h3 id="tips">Tips</h3>

<p>Suppose training contains \(N\) observations, and we select $m$ features in each splitting, then the <strong>time complexity</strong> for training \(B\) trees (each with depth \(d\)) is \(O(BNmd)\). The prediction time for a input is \(O(Bd)\).</p>

<p>For classification, we usually set $m$ as $⌊\sqrt{p}⌋$, and the minimum number of observations in each leaf or terminal node (minimum leaf node size) $1$.</p>

<p>For regression, we usually set $m$ as $⌊p/3⌋$, and the minimum leaf node size as $5$.</p>

<p><br /></p>

<p><strong>Reference:</strong></p>

<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. <em>The elements of statistical learning</em>. Vol. 1. No. 10. New York: Springer series in statistics, 2001.</p>

  </div>

  <br> </br>
  <p><font color="grey" size="4"> Comments </font></p>
  <HR color=#D1D0CE SIZE=10>

<div id="disqus_thread"></div>

<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://walkermao.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


                            

  
</div>

      </section>
    </main><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178951885-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178951885-1');
</script>

    
    <div id="back-top">
      <a href="javascript:void(0);" onclick="topFunction()" title="Back to top"> </a>
    </div>

  </body>
</html>

<script src = "/assets/js/scroll_into_view.js"></script>
<script src = "/assets/js/back_to_top.js"></script>