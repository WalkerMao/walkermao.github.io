<!DOCTYPE html>
<html lang="en">
  <head>
    <script src="https://kit.fontawesome.com/123ecac47c.js" crossorigin="anonymous"></script><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="shortcut icon" type="image/png" href="/assets/portfolio.png">
<title>The Curse of Dimensionality in Machine Learning</title>
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>The Curse of Dimensionality in Machine Learning | Weikai’s blog.</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="The Curse of Dimensionality in Machine Learning" />
<meta name="author" content="Weikai Mao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The curse of dimensionality, first introduced by Bellman 1, refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings. Bellman, Richard. “Dynamic programming.” Princeton University Press, 1957. &#8617;" />
<meta property="og:description" content="The curse of dimensionality, first introduced by Bellman 1, refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings. Bellman, Richard. “Dynamic programming.” Princeton University Press, 1957. &#8617;" />
<link rel="canonical" href="http://localhost:4000/the-curse-of-dimensionality-in-machine-learning.html" />
<meta property="og:url" content="http://localhost:4000/the-curse-of-dimensionality-in-machine-learning.html" />
<meta property="og:site_name" content="Weikai’s blog." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-07-17T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="The Curse of Dimensionality in Machine Learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Weikai Mao"},"dateModified":"2022-07-17T00:00:00+08:00","datePublished":"2022-07-17T00:00:00+08:00","description":"The curse of dimensionality, first introduced by Bellman 1, refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings. Bellman, Richard. “Dynamic programming.” Princeton University Press, 1957. &#8617;","headline":"The Curse of Dimensionality in Machine Learning","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/the-curse-of-dimensionality-in-machine-learning.html"},"url":"http://localhost:4000/the-curse-of-dimensionality-in-machine-learning.html"}</script>
<!-- End Jekyll SEO tag -->


<meta name="google-site-verification" content="wXp8C1QlYKCpKxfXyFfQXEv9l5fJvcOi53ofYmOcaSA" />
<meta name="msvalidate.01" content="97F0BB32D312B808156DE357EA8474D3" />
<meta name="yandex-verification" content="690106a82d8966ab" />

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ["$","$"], ["\\(","\\)"] ],
        displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno"
      }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>    


  







</head>
  <body>
    <main class="container">
      <section class="about">
        <a href="/" class="iconlink"> 
          <h3> 
            <i class="fas fa-home"></i> HOME 
          </h3> 
        </a>
        <a href="/"> <img src="/assets/portfolio.png" alt="Weikai Mao"></a>
        <h2 id="title"> Weikai Mao </h2>
        <p style="font-size:90%" class="tagline">maoweikai123@outlook.com</p>
        
        <ul class="social"><a href="https://github.com/WalkerMao">
              <li>
                <i class="fab fa-github"></i>
              </li>
            </a><a href="https://www.linkedin.com/in/weikai-mao-000249124">
              <li>
                <i class="fab fa-linkedin"></i>
              </li>
            </a><a href="/wechat-qr-code.html">
              <li>
                <i class="fab fa-weixin"></i>
              </li>
            </a></ul><h3 style="color:gray; font-weight:normal"> Categories: </h3>
            <ul class="post-categories" style="max-width:225px;">
              
                  <a class="post-link" href="/"> <li style="padding: 4px 8px;"> All </li> </a>
              
                  <a class="post-link" href="/categories/cs"> <li style="padding: 4px 8px;"> CS </li> </a>
              
                  <a class="post-link" href="/categories/cv"> <li style="padding: 4px 8px;"> CV </li> </a>
              
                  <a class="post-link" href="/categories/dl"> <li style="padding: 4px 8px;"> DL </li> </a>
              
                  <a class="post-link" href="/categories/fe"> <li style="padding: 4px 8px;"> FE </li> </a>
              
                  <a class="post-link" href="/categories/math"> <li style="padding: 4px 8px;"> Math </li> </a>
              
                  <a class="post-link" href="/categories/ml"> <li style="padding: 4px 8px;"> ML </li> </a>
              
                  <a class="post-link" href="/categories/nlp"> <li style="padding: 4px 8px;"> NLP </li> </a>
              
                  <a class="post-link" href="/categories/stat"> <li style="padding: 4px 8px;"> Stat </li> </a>
              
                  <a class="post-link" href="/categories/杂"> <li style="padding: 4px 8px;"> 杂 </li> </a>
              
            </ul><p>&copy; 2023 </p>

      </section>
      <section class="content">
        <div class="sidebar">
  <ul><li><a href="#model-estimation">Model Estimation</a></li><li><a href="#distances">Distances</a><ul><li><a href="#nearest-and-farthest-distance-tend-to-be-equal">Nearest and Farthest Distance Tend to Be Equal</a></li><li><a href="#fractional-distance-metrics-may-help">Fractional Distance Metrics May Help</a></li></ul></li><li><a href="#neighbors-are-not-local">Neighbors Are Not “Local”</a></li><li><a href="#close-to-borders">Close to Borders</a><ul><li><a href="#closest-distance-to-origin">Closest Distance to Origin</a></li><li><a href="#surface-area-to-volume-ratio">Surface Area to Volume Ratio</a></li><li><a href="#most-in-corners">Most in Corners</a></li></ul></li><li><a href="#others">Others</a><ul><li><a href="#gaussian-distribution">Gaussian Distribution</a></li></ul></li><li><a href="#references">References</a></li></ul>

</div><div class="post-container" id="viewpoint">
  <a class="post-link" href="/the-curse-of-dimensionality-in-machine-learning.html">
    <h2 class="post-title">The Curse of Dimensionality in Machine Learning</h2>
  </a>
  <div class="post-meta">
    <div>
      <ul class="post-categories"><a class="post-link" href="/categories/ml"> <li>ML</li> </a><a class="post-link" href="/categories/stat"> <li>Stat</li> </a></ul>
      <ul class="post-tags"></ul>
    </div>
    <div class="post-date">
      <i class="icon-calendar"></i>
      Jul 17, 2022
    </div>
  </div>
  <div style="line-height:77%;">
    <br>
  </div>
  <div class="post"><p>The curse of dimensionality, first introduced by Bellman <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings.</p>

<p>In machine learning, the curse of dimensionality is used interchangeably with the peaking phenomenon or Hughes phenomenon. This phenomenon states that with a fixed number of training samples, the average (expected) predictive power of a classifier or regressor first increases as the number of dimensions or features used is increased , but beyond a certain dimensionality it starts deteriorating. <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p>

<div align="center">
<figure>
<img src="https://miro.medium.com/max/1400/1*pcJdd2y924Xk61SXa7uYpw.jpeg" alt="img" style="zoom:60%; margin-bottom:-120px; overflow:hidden;" />
<figcaption style="font-size:80%;"> Figure: Hughes phenomenon. (<a href="https://towardsdatascience.com/curse-of-dimensionality-a-curse-to-machine-learning-c122ee33bfeb">Source</a>) </figcaption>
</figure>
</div>

<h2 id="model-estimation">Model Estimation</h2>

<p>With a fixed number of training samples and an increasing number of dimensions, the density of the training samples decreases exponentially, and the feature space becomes sparser and sparser. This may lead to:</p>

<ul>
  <li>
    <p>The statistical confidence and functionality of model parameters decreases.</p>
  </li>
  <li>
    <p>Model estimation tends to be overfitting, since that a classifier can easily find a separable hyperplane even for noise samples, and a regressor can easily fit noises as well.</p>
  </li>
  <li>
    <p>The complexity of functions of many variables can grow exponentially with the dimension, the number of samples needed to estimate such functions with a given level of accuracy grows exponentially as well. <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></p>
  </li>
</ul>

<h2 id="distances">Distances</h2>

<p>Under certain reasonable and broad assumptions on data distribution (much broader than independent and identically distributed dimensions), as dimensionality increases, given a sample set and an arbitrary query data point in that set, the distance to the nearest neighbor approaches the distance to the farthest neighbor. In other words, the contrast in distances to different data points becomes nonexistent. <sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">4</a></sup> Thus, the distance measures lose their effectiveness to measure similarity in a high-dimensional space.</p>

<p>The selection of distance metrics may help mitigate this curse. The paper <sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">5</a></sup> concluded that the \(L_p\)-norm distance metrics with lower \(p\) (e.g. less than 1) provide better contrast in distances between data points, and thus somewhat improve the effectiveness of distance metrics. However, later research <sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">6</a></sup> showed that this advantage decays with increasing dimension, and it showed that a greater relative contrast does not mean a better classification quality.</p>

<h3 id="nearest-and-farthest-distance-tend-to-be-equal">Nearest and Farthest Distance Tend to Be Equal</h3>

<p>As mentioned above, the nearest and farthest distance from the query point to the sample set tend to be equal as dimensionality increases. This is shown in Theorem 1 in <sup id="fnref:5:1" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">4</a></sup>, as decribied briefly below.</p>

<p>In a \(d\)-dimensional space, \(x_i = (x^{(1)}, \cdots, x^{(d)})^T \in \mathbb{R}^d,\; \forall i = 1, \cdots, n\). The distance between two points \(x_i\) and \(x_{i'}\) using \(L_p\)-norm (\(p &gt; 0\)) is</p>

\[\mathrm{dist}_{d,p}(x_i, x_{i'}) = \| x_i - x_{i'} \|_p = \left[ \sum_{j=1}^d \left( x_i^{(j)} - x_{i'}^{(j)} \right)^p \right]^{1/p}.\]

<p>Given a query point \(x_q\), denote the nearest and farthest distance from \(x_q\) to the \(n\) points as</p>

\[\mathrm{dist}^\min_{d,p} = \min_i \{ {\| x_i - x_q \|_p} \}, \\
\mathrm{dist}^\max_{d,p} = \max_i \{ {\| x_i - x_q \|_p} \}.\]

<p>If</p>

\[\lim_{d \to \infty} \mathrm{Var} \left( \frac{ \mathrm{dist}_{d,p}(x_i, x_q) }{E_{x_i} \left[ \mathrm{dist}_{d,p}(x_i, x_q) \right] } \right) = 0,\]

<p>then</p>

\[\frac{\mathrm{dist}^\max_{d,p}}{\mathrm{dist}^\min_{d,p}} \overset{\text{Prob}}{\to} 1, \; \text{as} \; d \to \infty,\]

<p>where \(E_{x_i}(\cdot)\) refers to expectation with respect to \(x_i\) (consider \(x_i\) as random), and \(\overset{\text{Prob}}{\to}\) refers to convergence in probability.</p>

<h3 id="fractional-distance-metrics-may-help">Fractional Distance Metrics May Help</h3>

<p>As mentioned above, the \(L_p\)-norm distance metrics with lower positive \(p\) (e.g. fractional distance metrics with \(p&lt;1\)) provide better contrast both in terms of the absolute difference \(\mathrm{dist}^\max_{d,p} - \mathrm{dist}^\min_{d,p}\) and relative difference \(\frac{ \mathrm{dist}^\max_{d,p} - \mathrm{dist}^\min_{d,p} }{ \mathrm{dist}^\min_{d,p} }\) of distances from points to a given query point, and thus somewhat mitigate the curse and improve the effectiveness of distance metrics.</p>

<p>As shown in Corollary 2 in <sup id="fnref:6:1" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">5</a></sup>, for \(n\) data points from an arbitrary distribution, we have</p>

\[c_p \leq \lim_{d \to \infty} E \left( \frac{\mathrm{dist}^\max_{d,p} - \mathrm{dist}^\min_{d,p}}{d^{1/p - 1/2}} \right) \leq (n-1) c_p,\]

<p>where \(c_p\) is some constant inversely proportional to \(p\), and  \(c_p = c (p+1)^{-1/p} (2p+1)^{-1/2}\) for uniform distribution and some constant \(c\).</p>

<p>This result shows that in high dimensional space \(\mathrm{dist}^\max_{d,p} - \mathrm{dist}^\min_{d,p}\) increases at the rate of \(d^{1/p−1/2}\), independent of the data distribution. This means that for \(p &lt; 2\), the value of this expression diverges to \(\infty\) (diverges faster for smaller \(p\)); for \(p=2\) (Euclidean distance metric), the expression is bounded by constants; whereas for \(p&gt;2\), it converges to 0 (converges faster for larger \(p\)).</p>

<p>Similarly, as shown in Corollary 4 in <sup id="fnref:6:2" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">5</a></sup>, for \(n\) data points from an arbitrary distribution, we have</p>

\[c'_p \leq \lim_{d \to \infty} E \left( \frac{\mathrm{dist}^\max_{d,p} - \mathrm{dist}^\min_{d,p}}{\mathrm{dist}^\min_{d,p}} \right) \sqrt{d} \leq (n-1) c'_p,\]

<p>where \(c'_p\) is some constant inversely proportional to \(p\), and  \(c'_p = c(2p+1)^{-1/2}\) for uniform distribution and some constant \(c\).</p>

<h2 id="neighbors-are-not-local">Neighbors Are Not “Local”</h2>

<p>In a high dimensional space, we need to cover a large range to capture just a few neighbors. Such neighbors are no longer “local”, and this makes the nearest neighbors be meaningless.</p>

<p>Consider \(n\) data points uniformly distributed in a \(d\)-dimensional unit hypercube or hypersphere. A hypercube or hypersphere that captures a fraction \(\alpha\) of the data has side length or radius \(\alpha^{1/d}\). That means we must cover of \(\alpha^{1/d}\) the range in each dimension. In ten dimensions, we need to cover 63% or 79% of the range of each input variable to capture only 1% or 10% or the data. <sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></p>

<div align="center">
<figure>
<img src="https://d3i71xaburhd42.cloudfront.net/581e4c6316feb4c18657a325afcdfd5524a4ead1/5-Figure2.6-1.png" alt="Figure 2.6 in ESL 2nd edition" style="zoom:100%;" />
<figcaption style="font-size:80%;"> Figure: The right figure shows the side-length of the subcube needed to capture a fraction r of the volume of the data,
for different dimensions d. (Source: [3]) </figcaption>
</figure>
</div>

<h2 id="close-to-borders">Close to Borders</h2>

<p>In a high dimensional space, data around the origin (the center of the hypercube or hypersphere) is much more sparse than data around the boundary/edge. Most data points (for both training and predicting) reside close to the borders (even in corners) of the feature space <sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">7</a></sup>. More surprisingly, data points are mostly closer to border than to any other data point <sup id="fnref:3:2" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>, for the reason showed in <a href="#neighbors-are-not-local">previous section</a> that neighbors are usually very far.</p>

<p>The reason that this presents a problem is that prediction is much more difficult near the boundaries of the training samples. For a prediction, we usually have to extrapolate from neighboring training data points rather than interpolate between them. <sup id="fnref:3:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></p>

<p>This property can be illustrated by following several aspects.</p>

<h3 id="closest-distance-to-origin">Closest Distance to Origin</h3>

<p>Consider \(n\) data points uniformly distributed in a \(d\)-dimensional unit ball centered at the origin. The expected median distance from the origin to the closest of \(n\) data points is</p>

\[\mathrm{dist}(d,n) = \left( 1 - 1/2^n \right)^{1/d}.\]

<p>This is the Exercise 2.3 in <sup id="fnref:3:4" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> and you can find a solution from <sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">8</a></sup>.</p>

<p>For example, \(\mathrm{dist}(10, 500) \approx 0.52\), which means we expect all data points reside closer to the boundaries (farthest distance is 0.48) than to the origin (closest distance is 0.52). Note that a hypersphere that captures a fraction \(1/500\) of the data has radius \((1/500)^{1/10} = 0.54 &gt; 0.48\). Hence most data points are closer to the border of the feature space than to any other data point.</p>

<h3 id="surface-area-to-volume-ratio">Surface Area to Volume Ratio</h3>

<p>The ratio increases linearly with dimension.</p>

<p><strong>Hypercube:</strong></p>

<p>For a \(d\)-dimensional cube of side length \(r\), the surface area and volume are</p>

\[S_d = 2dr^{d-1},\; V_d = r^d.\]

<p>So the ratio for a hypercube is \(S_d/V_d = 2d/r\).</p>

<p><strong>Hypersphere:</strong></p>

<p>For a \(d\)-dimensional sphere/ball of radius \(r\), the surface area and volume are</p>

\[S_d = \frac{d r^{d-1} \pi^{d/2}}{\Gamma(1+d/2)} = \frac{d r^{d-1} \pi^{d/2}}{(d/2)!},\\
V_d = \frac{r^d \pi^{d/2}}{\Gamma(1+d/2)} = \frac{r^d \pi^{d/2}}{(d/2)!}.\]

<p>So the ratio for a hypersphere is \(S_d/V_d = d/r\).</p>

<p>Thus the surface area to volume ratio either for a hypercube or hypersphere goes to infinity as dimension increases:</p>

\[S_d/V_d \propto d/r \to \infty, \; \text{as} \; d \to \infty.\]

<p>This conclusion illustrates that most of the volume is contained in an shell or annulus of width proportional to \(r/d\). This means that almost all of the high dimensional box/orange’s mass is in the shell/peel. <sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">9</a></sup></p>

<p>By the way, another interesting surprise in high-dimensional spaces is that, the surface area and volume of a \(d\)-dimensional sphere of given radius both go (very quickly) to 0 as the dimension \(d\) increases to infinity. Here is a simple derivation: By Stirling’s formula, \((d/2)! \sim \sqrt{\pi d} (\frac{d}{2e})^d\), then we have</p>

\[S_d \to 0,\, V_d \to 0, \; \text{as} \; d \to \infty.\]

<div align="center">
<figure>
<img src="https://i.stack.imgur.com/wDUGr.png" alt="unit sphere surface area" style="zoom: 57%;" /> <img src="https://i.stack.imgur.com/ZI8xd.png" alt="unit ball volume" style="zoom:58.5%;" />
<figcaption style="font-size:80%;"> Figure: Surface area and volume of a unit hypersphere go to 0 as dimension increases. (<a href="https://math.stackexchange.com/q/2601105">Source</a>) </figcaption>
</figure>
</div>

<h3 id="most-in-corners">Most in Corners</h3>

<p>Most of the volume of a high-dimensional cube is located in its corners. <sup id="fnref:8:1" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">9</a></sup></p>

<p>We assume that data points are uniformaly distributed in a hypercube given by \([−r, r]^d\). By Chernoff’s Inequality, The probability that a data point \(x = (x^{(1)}, \cdots, x^{(d)})^T \in \mathbb{R}^d\) reside in the inscribed hypersphere of raidus \(r\) is</p>

\[P\left(\|x\|_2 \leq r \right) = P\left(\sqrt{\sum_{j=1}^d {x^{(j)}}^2} \leq r \right) \leq e^{-d/10}.\]

<p>Since this probability converges to 0 as the dimension \(d\) goes to infinity, this shows random points in high cubes are most likely outside the sphere. In other words, almost all the volume of hypercubes lie in their corners.</p>

<div align="center">
<figure>
<img src="https://www.visiondummy.com/wp-content/uploads/2014/04/sparseness.png" alt="Highly dimensional feature spaces are sparse around their origin" style="zoom:100%;" />
<figcaption style="font-size:80%;"> Figure: As the dimensionality increases, a larger percentage of the data points reside in the corners of the feature space. (Source: [4]) </figcaption>
</figure>
</div>

<p>In addition, an interesting surprise in high-dimensional spaces is that hypercubes are both convex and “pointy”. <sup id="fnref:8:2" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">9</a></sup></p>

<h2 id="others">Others</h2>

<h3 id="gaussian-distribution">Gaussian Distribution</h3>

<p>Gaussian distribution become flat and heavy tailed distributions in high dimensional spaces, such that the minimum and maximum gaussian likelihood tend to be equal. <sup id="fnref:4:1" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">7</a></sup></p>

<p><br /></p>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Bellman, Richard. “Dynamic programming.” <em>Princeton University Press</em>, 1957. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Wikipedia contributors. “<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">Curse of dimensionality</a>.” <em>Wikipedia, The Free Encyclopedia</em>, 2022. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Hastie, Trevor, et al. Section 2.5: Local methods in high dimensions. “The elements of statistical learning: data mining, inference, and prediction.” Vol. 2. <em>New York: springer</em>, 2009. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:3:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:3:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a> <a href="#fnref:3:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Beyer, Kevin, et al. “<a href="http://www.loria.fr/~berger/Enseignement/Master2/Exposes/beyer.pdf">When is “nearest neighbor” meaningful?</a>.” <em>International conference on database theory</em>. Springer, Berlin, Heidelberg, 1998. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:5:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Aggarwal, Charu C., Alexander Hinneburg, and Daniel A. Keim. “<a href="https://bib.dbvis.de/uploadedFiles/155.pdf">On the surprising behavior of distance metrics in high dimensional space</a>.” <em>International conference on database theory</em>. Springer, Berlin, Heidelberg, 2001. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:6:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:6:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>Mirkes, Evgeny M., Jeza Allohibi, and Alexander Gorban. “<a href="https://www.mdpi.com/1099-4300/22/10/1105">Fractional norms and quasinorms do not help to overcome the curse of dimensionality</a>.” <em>Entropy</em> 22.10 (2020): 1105. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Spruyt, Vincent. “<a href="https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/">The curse of dimensionality in classification</a>.” <em><a href="https://www.visiondummy.com/">Computer vision for dummies</a></em>, 2014. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:4:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>Weatherwax, John L., and Epstein, David. “<a href="https://waxworksmath.com/Authors/G_M/Hastie/WriteUp/Weatherwax_Epstein_Hastie_Solution_Manual.pdf">A solution manual and notes for: The elements of statistical learning.</a>” 2021. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>Strohmer, Thomas. “<a href="https://www.math.ucdavis.edu/~strohmer/courses/180BigData/180lecture1.pdf">Surprises in high dimensions</a>.”, 2017. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:8:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:8:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
  </ol>
</div>

  </div>

  <br> </br>
  <p><font color="grey" size="4"> Comments </font></p>
  <HR color=#D1D0CE SIZE=10>

<div id="disqus_thread"></div>

<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://walkermao.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


                            

  
</div>

      </section>
    </main><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178951885-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178951885-1');
</script>

    
    <div id="back-top">
      <a href="javascript:void(0);" onclick="topFunction()" title="Back to top"> </a>
    </div>

  </body>
</html>

<script src = "/assets/js/scroll_into_view.js"></script>
<script src = "/assets/js/back_to_top.js"></script>