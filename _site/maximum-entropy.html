<!DOCTYPE html>
<html lang="en">
  <head>
    <script src="https://kit.fontawesome.com/123ecac47c.js" crossorigin="anonymous"></script><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="shortcut icon" type="image/png" href="/assets/portfolio.png">
<title>Maximum Entropy</title>
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Maximum Entropy | Weikai’s blog.</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Maximum Entropy" />
<meta name="author" content="Weikai Mao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Upper Bound of Entropy" />
<meta property="og:description" content="Upper Bound of Entropy" />
<link rel="canonical" href="http://localhost:4000/maximum-entropy.html" />
<meta property="og:url" content="http://localhost:4000/maximum-entropy.html" />
<meta property="og:site_name" content="Weikai’s blog." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-07T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Maximum Entropy" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Weikai Mao"},"dateModified":"2020-06-07T00:00:00+08:00","datePublished":"2020-06-07T00:00:00+08:00","description":"Upper Bound of Entropy","headline":"Maximum Entropy","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/maximum-entropy.html"},"url":"http://localhost:4000/maximum-entropy.html"}</script>
<!-- End Jekyll SEO tag -->


<meta name="google-site-verification" content="wXp8C1QlYKCpKxfXyFfQXEv9l5fJvcOi53ofYmOcaSA" />
<meta name="msvalidate.01" content="97F0BB32D312B808156DE357EA8474D3" />
<meta name="yandex-verification" content="690106a82d8966ab" />

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ["$","$"], ["\\(","\\)"] ],
        displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno"
      }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>    


  







</head>
  <body>
    <main class="container">
      <section class="about">
        <a href="/" class="iconlink"> 
          <h3> 
            <i class="fas fa-home"></i> HOME 
          </h3> 
        </a>
        <a href="/"> <img src="/assets/portfolio.png" alt="Weikai Mao"></a>
        <h2 id="title"> Weikai Mao </h2>
        <p style="font-size:90%" class="tagline">maoweikai123@outlook.com</p>
        
        <ul class="social"><a href="https://github.com/WalkerMao">
              <li>
                <i class="fab fa-github"></i>
              </li>
            </a><a href="https://www.linkedin.com/in/weikai-mao-000249124">
              <li>
                <i class="fab fa-linkedin"></i>
              </li>
            </a><a href="/wechat-qr-code.html">
              <li>
                <i class="fab fa-weixin"></i>
              </li>
            </a></ul><h3 style="color:gray; font-weight:normal"> Categories: </h3>
            <ul class="post-categories" style="max-width:225px;">
              
                  <a class="post-link" href="/"> <li style="padding: 4px 8px;"> All </li> </a>
              
                  <a class="post-link" href="/categories/cs"> <li style="padding: 4px 8px;"> CS </li> </a>
              
                  <a class="post-link" href="/categories/cv"> <li style="padding: 4px 8px;"> CV </li> </a>
              
                  <a class="post-link" href="/categories/dl"> <li style="padding: 4px 8px;"> DL </li> </a>
              
                  <a class="post-link" href="/categories/fe"> <li style="padding: 4px 8px;"> FE </li> </a>
              
                  <a class="post-link" href="/categories/math"> <li style="padding: 4px 8px;"> Math </li> </a>
              
                  <a class="post-link" href="/categories/ml"> <li style="padding: 4px 8px;"> ML </li> </a>
              
                  <a class="post-link" href="/categories/nlp"> <li style="padding: 4px 8px;"> NLP </li> </a>
              
                  <a class="post-link" href="/categories/stat"> <li style="padding: 4px 8px;"> Stat </li> </a>
              
                  <a class="post-link" href="/categories/杂"> <li style="padding: 4px 8px;"> 杂 </li> </a>
              
            </ul><p>&copy; 2023 </p>

      </section>
      <section class="content">
        <div class="sidebar">
  <ul><li><a href="#upper-bound-of-entropy">Upper Bound of Entropy</a></li><li><a href="#principle-of-maximum-entropy">Principle of Maximum Entropy</a></li><li><a href="#example-bergers-burgers">Example: Berger’s Burgers</a></li><li><a href="#maximum-entropy-classifier">Maximum Entropy Classifier</a></li></ul>

</div><div class="post-container" id="viewpoint">
  <a class="post-link" href="/maximum-entropy.html">
    <h2 class="post-title">Maximum Entropy</h2>
  </a>
  <div class="post-meta">
    <div>
      <ul class="post-categories"><a class="post-link" href="/categories/cs"> <li>CS</li> </a><a class="post-link" href="/categories/stat"> <li>Stat</li> </a></ul>
      <ul class="post-tags"></ul>
    </div>
    <div class="post-date">
      <i class="icon-calendar"></i>
      Jun 7, 2020
    </div>
  </div>
  <div style="line-height:77%;">
    <br>
  </div>
  <div class="post"><h2 id="upper-bound-of-entropy">Upper Bound of Entropy</h2>

<p>For a finite sample space, <strong>uniform distribution has maximum entropy among all distributions</strong>.</p>

<p>Given a random variable \(X\) with sample space \(\mathcal{X}\) and outcomes \(x \in \mathcal{X}\). Suppose \(X\) follows the distribution \(P_X\). Denote \(\mid\mathcal{X}\mid\) as the number of elements in the set \(\mathcal{X}\),</p>

<p><strong>Theorem.</strong> \(H(X)\leq\log\mid\mathcal{X}\mid\). Equality iff $X$ has uniform distribution on \(\mathcal{X}\).</p>

<p>Proof:</p>

<p>Denote \(U_{\mathcal{X}}\) as the uniform distribution on the sample space \(\mathcal{X}\), then we have \(U_{\mathcal{X}}(x)=\frac{1}{\mid\mathcal{X}\mid}\).</p>

<p>By the non-negativity of KL divergence, or equivalently by Gibbs’ inequality,</p>

\[\begin{align}
0 &amp;\leq D_{\text{KL}}(P_X \parallel U_{\mathcal{X}}) = \sum_{x \in \mathcal{X}} P_X(x)\log\frac{P_X(x)}{U_{\mathcal{X}}(x)} \\
&amp;= \sum_{x \in \mathcal{X}} P_X(x) \log |\mathcal{X}| - \left(-\sum_{x \in \mathcal{X}} P_X(x)\log P_X(x) \right) \\ 
&amp;= \log |\mathcal{X}| - H(X).
\end{align}\]

<p>Equality iff \(P_X \equiv U_{\mathcal{X}}\). \(□\)</p>

<p>Note that entropy is different from variance. Variance is the expectation of the squared deviation of a random variable from its mean. Also note that the properties of the entropy of continuous distribution is different from that of discrete distribution.</p>

<div align="center"> <img src="../../../pictures/variance-and-entropy.png" alt="variance-and-entropy" style="zoom:60%;" /> </div>

<h2 id="principle-of-maximum-entropy">Principle of Maximum Entropy</h2>

<p>Bayesian methods usually need to assume the prior distribution. Naturally we want to avoid inadvertently assuming more knowledge than we actually have, and the <strong>principle of maximum entropy</strong> is the technique for doing this, which states that <strong>the probability distribution which best represents the current state of knowledge is the one with largest entropy</strong>.</p>

<p>Suppose the outcomes of random variable $X$ are \(\mathcal{X}=\{x_i\}_{i=1}^n\), then entropy is \(H(X)=-\sum_{i=1}^n P(x_i) \log_b P(x_i)\). (Common values of the logarithm base $b$ are $2$, and the corresponding units of entropy are the bits for \(b=2\)). Now we want to estimate the probabilities \(P(x_1),P(x_2),\cdots,P(x_n)\) given $m$ constraints \(\sum_{i=1}^n P(x_i)g_j(x_i)=E\left[g_j(X)\right], j=1,2,\cdots,m\), where \(g_j(x_i)\) and \(E\left[g_j(X)\right]\) are all known for each \(i,j\).</p>

<p>The <strong>formulation</strong> of maximizing entropy is</p>

\[P(x_1),\cdots,P(x_n) = \underset{P(x_1),\cdots,P(x_n)}{\text{argmax }} H(X) = \underset{P(x_1),\cdots,P(x_n)}{\text{argmax }} -\sum_{i=1}^n P(x_i) \log_b P(x_i), \\
\begin{align}
\text{subject to } \\
&amp; P(x_i) \geq 0 \text{ for } i=1,\cdots,n, \\ &amp;\sum_{i=1}^{n}P(x_i)=1, \\ 
&amp;\sum_{i=1}^n P(x_i)g_j(x_i)=E\left[g_j(X)\right] \text{ for } j=1,\cdots,m.
\end{align}\]

<p>We can apply the method of <strong>Lagrange multipliers</strong> to solve this maximization problem. The Lagrangian function is</p>

\[\begin{align}
\mathcal{L}\big(P(x_1),\cdots,P(x_n),\lambda_0,\cdots,\lambda_m\big) = &amp;-\sum_{i=1}^n P(x_i) \log_b P(x_i) + \lambda_0 \left(\sum_{i=1}^{n}P(x_i)-1\right) \\ &amp;+ \sum_{j=1}^{m} \lambda_j \left( \sum_{i=1}^n P(x_i)g_j(x_i) - E\left[g_j(X)\right] \right).
\end{align}\]

<p>Take derivative with respect to \(P(x_i)\) and set to \(0\), we have</p>

\[\frac{\partial \mathcal{L}\big(P(x_1),\cdots,P(x_n),\lambda_0,\cdots,\lambda_m\big)}{\partial P(x_1)} = 0 \implies P^*(x_i) = b^{\lambda_0 - 1 + \sum_{j=1}^m \lambda_jE\left[g_j(X)\right]}.\]

<p>The Lagrange multipliers \(\lambda_0,\lambda_1,\cdots,\lambda_m\) are chosen such that</p>

\[\begin{cases}
\sum_{i=1}^{n}P(x_i)=1, \\
\sum_{i=1}^n P(x_i)g_j(x_i)=E\left[g_j(X)\right] \text{ for } j=1,\cdots,m.
\end{cases}\]

<p>Since we have \(m+1\) constraints, and it is equal to the number of Lagrange multipliers, thus we can get the exact solution of \(\lambda_0,\lambda_1,\cdots,\lambda_m\).</p>

<p>Note that the logarithm base $b$ does not influence the result.</p>

<h2 id="example-bergers-burgers">Example: Berger’s Burgers</h2>

<p>Walker usually buys meals from a fast food restaurant, Berger’s Burger, which offers four different meals as shown below:</p>

<table>
  <thead>
    <tr>
      <th>Item</th>
      <th>Price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Burger (\(x_1\))</td>
      <td>\($1\)</td>
    </tr>
    <tr>
      <td>Chicken (\(x_2\))</td>
      <td>\($2\)</td>
    </tr>
    <tr>
      <td>Fish (\(x_3\))</td>
      <td>\($3\)</td>
    </tr>
    <tr>
      <td>Tofu (\(x_4\))</td>
      <td>\($8\)</td>
    </tr>
  </tbody>
</table>

<p>Now Walker tells you that his average cost of each meal is \($2.5\). What is the frequency that each item being ordered? We can solve this problem by the technique we discussed in the previous section.</p>

<p>Denote the item he orders as a random variable \(X\), which has possible outcomes \(x_1,x_2,x_3,x_4\). The entropy of \(X\) is \(H(X)=-\sum_{i=1}^4 P(x_i) \log_2 P(x_i)\).</p>

<p>The formulation of maximizing entropy is</p>

\[P(x_1),\cdots,P(x_4) = \underset{P(x_1),\cdots,P(x_4)}{\text{argmax }} H(X) = \underset{P(x_1),\cdots,P(x_4)}{\text{argmax }} -\sum_{i=1}^4 P(x_i) \log_b P(x_i), \\
\begin{align}
\text{subject to } \\
&amp; P(x_i) \geq 0 \text{ for } i=1,\cdots,4, \\ &amp;\sum_{i=1}^{4}P(x_i)=1, \\ 
&amp; P(x_1)+2P(x_2)+3P(x_3)+8P(x_4)=2.5.
\end{align}\]

<p>By the method of Lagrange multipliers, we have</p>

\[P^*(x_1)=2^{\lambda_0 - 1 + \lambda_1}, P^*(x_2)=2^{\lambda_0 - 1 + 2\lambda_1}, P^*(x_3)=2^{\lambda_0 - 1 + 3\lambda_1}, P^*(x_4)=2^{\lambda_0 - 1 + 8\lambda_1}.\]

<p>Plug these equations into the constraints</p>

\[\begin{cases}
P(x_1)+P(x_2)+P(x_3)+P(x_4)=1, \\
P(x_1)+2P(x_2)+3P(x_3)+8P(x_4)=2.5.
\end{cases}\]

<p>We have two constraints and two variables \(\lambda_0,\lambda_1\), thus we can get the exact solution:</p>

\[\lambda_0 = 1.2371, \lambda_1 = 0.2586.\]

<p>Plug them back to \(P^*(x_1),\cdots,P^*(x_4)\) and we have</p>

\[P^*(x_1) = 0.3546, P^*(x_2)=0.2964, P^*(x_3)=0.2478, P^*(x_4) = 0.1011.\]

<h2 id="maximum-entropy-classifier">Maximum Entropy Classifier</h2>

<p><strong>The maximum entropy classifier is equivalent to logistic regression.</strong> Let’s derive logistic regression by maximum entropy to show why they are equivalent.</p>

<p>For \(K\)-class classification problem, denote $k$ as the index of class, vector \(x_i\) with dimension \(p \times 1\) as the $i$-th observation of the input variables, \(x_i^{(j)}\) as the $i$-th observation of the \(j\)-th input variable ($j$-th element in vector \(x_i\)), scalar \(y_i\) as the $i$-th observation of the output variable, and \(\hat{p}_{ik}\) as the estimated probability of the $i$-th observation is of class $k$.</p>

<p>Remember that if we optimize logistic regression by maximum likelihood, we can derive the following conclusion: 
\(\sum_{i=1}^{n} \hat{p}_{ik} x_i^{(j)} = \sum_{i=1}^n \mathbf{1}(y_i=k)x_i^{(j)} \text{ for any } k, j.\)
We call this equation the “balance equation”, which indicates, for any \(k,j\), the sum of any feature $j$ of training data $x_i$’s in a particular class $k$ is equal to the sum of probability mass the model places in that feature summed across all data.</p>

<p>In the equivalent maximum entropy derivation of logistic regression we don’t have to cleverly guess the sigmoid form. Instead we assume we want the “balance equation” to be true and then we can solve for the form for \(\hat{p}_{ik}\).</p>

<p>The entropy of the estimated probabilities \(\hat{p}_{ik}\)’s is \(-\sum_{k=1}^K \sum_{i=1}^n \hat{p}_{ik} \log\hat{p}_{ik}\).</p>

<p>The formulation of maximizing entropy is</p>

\[\hat{p}_{ik}\text{'s} = \underset{\hat{p}_{ik}\text{'s}}{\text{argmax }} -\sum_{k=1}^K \sum_{i=1}^n \hat{p}_{ik} \log\hat{p}_{ik}, \\
\begin{align}
\text{subject to } \\
&amp; \hat{p}_{ik} \geq 0 \text{ for any }i,k, \\ 
&amp; \sum_{k=1}^{K}\hat{p}_{ik}=1, \\ 
&amp; \sum_{i=1}^{n} \hat{p}_{ik} x_i^{(j)} = \sum_{i=1}^n \mathbf{1}(y_i=k)x_i^{(j)} \text{ for any } k, j.
\end{align}\]

<p>The constraint \(\hat{p}_{ik} \geq 0\) are hard to work with because it is an inequality instead of an equality, so we leave it out for now and hope that what we find can be shown to satisfy it.</p>

<p>We still apply the method of Lagrange multipliers to solve this maximization problem. The Lagrangian function is</p>

\[\begin{align}
\mathcal{L} \big( \hat{p}_{ik}\text{'s}, \lambda_k^{(j)}\text{'s}, \mu_i\text{'s} \big) 
=  &amp;- \sum_{k=1}^K \sum_{i=1}^n \hat{p}_{ik} \log\hat{p}_{ik} + \sum_{i=1}^{n}\mu_i\left( \sum_{k=1}^{K}\hat{p}_{ik} - 1 \right) \\
&amp;+ \sum_{j=1}^p\sum_{k=1}^{K} \lambda_k^{(j)} \left( \sum_{i=1}^{n} \hat{p}_{ik} x_i^{(j)} - \sum_{i=1}^n \mathbf{1}(y_i=k)x_i^{(j)} \right).
\end{align}\]

<p>Take derivative of the Lagrangian function with respect to \(\hat{p}_{ik}\) for all \(i,k\) and set to \(0\), we have</p>

\[\begin{align}
&amp; \frac{\partial \mathcal{L}}{\partial \hat{p}_{ik}} = \sum_{j=1}^p \lambda_k^{(j)} x_i^{(j)} + \mu_i - \log\hat{p}_{ik} - 1 = 0 \\ 
&amp;\implies \hat{p}_{ik} = \exp\left(\sum_{j=1}^p \lambda_k^{(j)} x_i^{(j)} + \mu_i - 1\right)
\end{align}\]

<p>Denote \(\lambda_k := \left(\lambda_k^{(1)}, \lambda_k^{(2)}, \cdots, \lambda_k^{(p)}\right)^T\), then</p>

\[\hat{p}_{ik} = \exp\left(\lambda_k^T x_i + \mu_i - 1\right).\]

<p>We can see that this form satisfy the constraint \(\hat{p}_{ik} \geq 0\).</p>

<p>Now we need to solve for \(\lambda_k\) and \(\mu_i\) by the constraints.</p>

\[\sum_{k=1}^{K}\hat{p}_{ik} = \sum_{k=1}^{K}\exp\left(\lambda_k^T x_i + \mu_i - 1\right) = 1 \implies e^{\mu_i} = 1 / \sum_{k=1}^{K} e^{\lambda_k^T x_i - 1}.\]

<p>We then plug it back and simplify to get</p>

\[\hat{p}_{ik} = \frac{e^{\lambda_k^T x_i}}{\sum_{k=1}^{K} e^{\lambda_k^T x_i}}.\]

<p>This is exact the softmax function (or multi-category version of the sigmoid function), and thus we showed that the maximum entropy classifier is equivalent to logistic regression.</p>

<p><br /></p>

<p><strong>References:</strong></p>

<p>Xie, Yao. (2010, Dec 9). <em>Chain rules and inequalities</em>. Retrieved June 4, 2020, from https://www2.isye.gatech.edu/~yxie77/ece587/Lecture2.pdf.</p>

<p>Xie, Yao. (2010, Dec 9). <em>Maximum entropy</em>. Retrieved June 6, 2020, from https://www2.isye.gatech.edu/~yxie77/ece587/Lecture11.pdf.</p>

<p>Paul Penfield, Jr. (2004, Apr 2). <em>Principle of Maximum Entropy</em>. Retrieved June 7, 2020, from http://www-mtl.mit.edu/Courses/6.050/notes/chapter9.pdf.</p>

<p>Mount, John. (2011, Sep 23). <em>The equivalence of logistic regression and maximum entropy models</em>. Retrieved June 7, 2020, from https://pdfs.semanticscholar.org/19cc/c9e2937b3260ac2c93020174c09c2891672e.pdf.</p>

  </div>

  <br> </br>
  <p><font color="grey" size="4"> Comments </font></p>
  <HR color=#D1D0CE SIZE=10>

<div id="disqus_thread"></div>

<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://walkermao.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


                            

  
</div>

      </section>
    </main><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178951885-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178951885-1');
</script>

    
    <div id="back-top">
      <a href="javascript:void(0);" onclick="topFunction()" title="Back to top"> </a>
    </div>

  </body>
</html>

<script src = "/assets/js/scroll_into_view.js"></script>
<script src = "/assets/js/back_to_top.js"></script>