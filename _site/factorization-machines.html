<!DOCTYPE html>
<html lang="en">
  <head>
    <script src="https://kit.fontawesome.com/123ecac47c.js" crossorigin="anonymous"></script><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="shortcut icon" type="image/png" href="/assets/portfolio.png">
<title>Factorization Machines</title>
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Factorization Machines | Weikai’s blog.</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Factorization Machines" />
<meta name="author" content="Weikai Mao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Factorization machines (FMs) were first introduced by Steffen Rendle in 2010. FMs model all interactions between features using factorized parameters. Thus they are able to estimate the weights (coefficients) of feature interactions even in problems with huge sparsity (like recommender systems)." />
<meta property="og:description" content="Factorization machines (FMs) were first introduced by Steffen Rendle in 2010. FMs model all interactions between features using factorized parameters. Thus they are able to estimate the weights (coefficients) of feature interactions even in problems with huge sparsity (like recommender systems)." />
<link rel="canonical" href="http://localhost:4000/factorization-machines.html" />
<meta property="og:url" content="http://localhost:4000/factorization-machines.html" />
<meta property="og:site_name" content="Weikai’s blog." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-01T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Factorization Machines" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Weikai Mao"},"dateModified":"2020-12-01T00:00:00+08:00","datePublished":"2020-12-01T00:00:00+08:00","description":"Factorization machines (FMs) were first introduced by Steffen Rendle in 2010. FMs model all interactions between features using factorized parameters. Thus they are able to estimate the weights (coefficients) of feature interactions even in problems with huge sparsity (like recommender systems).","headline":"Factorization Machines","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/factorization-machines.html"},"url":"http://localhost:4000/factorization-machines.html"}</script>
<!-- End Jekyll SEO tag -->


<meta name="google-site-verification" content="wXp8C1QlYKCpKxfXyFfQXEv9l5fJvcOi53ofYmOcaSA" />
<meta name="msvalidate.01" content="97F0BB32D312B808156DE357EA8474D3" />
<meta name="yandex-verification" content="690106a82d8966ab" />

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ["$","$"], ["\\(","\\)"] ],
        displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno"
      }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>    


  







</head>
  <body>
    <main class="container">
      <section class="about">
        <a href="/" class="iconlink"> 
          <h3> 
            <i class="fas fa-home"></i> HOME 
          </h3> 
        </a>
        <a href="/"> <img src="/assets/portfolio.png" alt="Weikai Mao"></a>
        <h2 id="title"> Weikai Mao </h2>
        <p style="font-size:90%" class="tagline">maoweikai123@outlook.com</p>
        
        <ul class="social"><a href="https://github.com/WalkerMao">
              <li>
                <i class="fab fa-github"></i>
              </li>
            </a><a href="https://www.linkedin.com/in/weikai-mao-000249124">
              <li>
                <i class="fab fa-linkedin"></i>
              </li>
            </a><a href="/wechat-qr-code.html">
              <li>
                <i class="fab fa-weixin"></i>
              </li>
            </a></ul><h3 style="color:gray; font-weight:normal"> Categories: </h3>
            <ul class="post-categories" style="max-width:225px;">
              
                  <a class="post-link" href="/"> <li style="padding: 4px 8px;"> All </li> </a>
              
                  <a class="post-link" href="/categories/cs"> <li style="padding: 4px 8px;"> CS </li> </a>
              
                  <a class="post-link" href="/categories/cv"> <li style="padding: 4px 8px;"> CV </li> </a>
              
                  <a class="post-link" href="/categories/dl"> <li style="padding: 4px 8px;"> DL </li> </a>
              
                  <a class="post-link" href="/categories/fe"> <li style="padding: 4px 8px;"> FE </li> </a>
              
                  <a class="post-link" href="/categories/math"> <li style="padding: 4px 8px;"> Math </li> </a>
              
                  <a class="post-link" href="/categories/ml"> <li style="padding: 4px 8px;"> ML </li> </a>
              
                  <a class="post-link" href="/categories/nlp"> <li style="padding: 4px 8px;"> NLP </li> </a>
              
                  <a class="post-link" href="/categories/stat"> <li style="padding: 4px 8px;"> Stat </li> </a>
              
                  <a class="post-link" href="/categories/杂"> <li style="padding: 4px 8px;"> 杂 </li> </a>
              
            </ul><p>&copy; 2023 </p>

      </section>
      <section class="content">
        <div class="sidebar">
  <ul><li><a href="#linear-model-with-interactions">Linear Model with Interactions</a></li><li><a href="#derivation-of-factorization-machines">Derivation of Factorization Machines</a></li><li><a href="#learning-factorization-machines">Learning Factorization Machines</a></li><li><a href="#tips-and-summary">Tips and Summary</a></li></ul>

</div><div class="post-container" id="viewpoint">
  <a class="post-link" href="/factorization-machines.html">
    <h2 class="post-title">Factorization Machines</h2>
  </a>
  <div class="post-meta">
    <div>
      <ul class="post-categories"><a class="post-link" href="/categories/fe"> <li>FE</li> </a><a class="post-link" href="/categories/ml"> <li>ML</li> </a></ul>
      <ul class="post-tags"></ul>
    </div>
    <div class="post-date">
      <i class="icon-calendar"></i>
      Dec 1, 2020
    </div>
  </div>
  <div style="line-height:77%;">
    <br>
  </div>
  <div class="post"><p>Factorization machines (FMs) were first introduced by Steffen Rendle in 2010. FMs model all interactions between features using factorized parameters. Thus they are able to estimate the weights (coefficients) of feature interactions even in problems with huge sparsity (like recommender systems).</p>

<h2 id="linear-model-with-interactions">Linear Model with Interactions</h2>

<p>Suppose we have \(p\) features \(x_1,\cdots,x_p\). A linear model with no interactions can be expressed as</p>

\[\hat{y} = w_0 + \sum_{j=1}^p w_jx_j. \tag {1}\]

<p>Sometimes we want to model the interactions between features, then a linear model with degree \(d=2\) interactions can be expressed as</p>

\[\hat{y} = w_0 + \sum_{j=1}^p w_jx_j + \sum_{j=1}^p\sum_{j'=j+1}^p w_{jj'}x_jx_{j'}. \tag {2}\]

<p>There are \(\frac{p(p-1)}{2}\) weights \(w_{jj'}\) of interactions to estimate. Estimating too many weights may lead to overfitting. For sparse features, most of feature values are zero, not to mention the interactions, thus we do not have enough data to estimate the weights of these interactions.</p>

<h2 id="derivation-of-factorization-machines">Derivation of Factorization Machines</h2>

<p>The model equation for a factorization machine of degree \(d=2\) is defined as</p>

\[\hat{y} = w_0 + \sum_{j=1}^p w_jx_j + \sum_{j=1}^p\sum_{j'=j+1}^p \langle v_j, v_{j'} \rangle x_jx_{j'}. \tag {3}\]

<p>Now let’s derive that</p>

\[w_{jj'} = v_j^Tv_{j'} = \langle v_j, v_{j'} \rangle. \tag {4}\]

<p>We firstly compose the weights of interactions as a matrix</p>

\[W = \begin{pmatrix}
  w_{11} &amp; w_{12} &amp; \cdots &amp; w_{1p} \\
  w_{21} &amp; w_{22} &amp; \cdots &amp; w_{2p} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
  w_{p1} &amp; w_{p2} &amp; \cdots &amp; w_{pp}
 \end{pmatrix}. 
 \tag {5}\]

<p>Note that \(w_{jj'}=w_{j'j}\) for all \(j,j'=1,2,\cdots,p\), and \(W\) is a symmetric matrix.</p>

<p>We assume \(W\) is positive semidefinite. It is well known that for any positive semidefinite matrix \(W\in\mathbb{R}^{p \times p}\), there exists a matrix \(V \in \mathbb{R}^{k \times p}\) such that \(W = V^TV\) provided that \(k\) is sufficiently large.</p>

<p>Denote \(v_j = (v_{1j}, v_{2j}, \cdots, v_{kj})^T \in \mathbb{R}^{k}\) as the \(j\)-th column vector of the matrix \(V\). Then we have</p>

\[V = 
\begin{pmatrix}
  v_1, v_2, \cdots, v_p
\end{pmatrix} 
= 
\begin{pmatrix}
  v_{11} &amp; v_{12} &amp; \cdots &amp; v_{1p} \\
  v_{21} &amp; v_{22} &amp; \cdots &amp; v_{2p} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
  v_{k1} &amp; v_{k2} &amp; \cdots &amp; v_{kp}
\end{pmatrix}.
\tag {6}\]

<p>Rather than estimating all \(\frac{p(p-1)}{2}\) weights of interactions independently, we estimate all \(kp\) entries in \(V\).</p>

<p>Now we decompose \(W\) as</p>

\[W = V^TV = 
\begin{pmatrix}
  v_1^T \\
  v_2^T \\
  \cdots  \\
  v_p^T
\end{pmatrix} 
\begin{pmatrix}
  v_1, v_2, \cdots, v_p
\end{pmatrix} 
= 
\begin{pmatrix}
  v_{1}^Tv_{1} &amp; v_{1}^Tv_{2} &amp; \cdots &amp; v_{1}^Tv_{p} \\
  v_{2}^Tv_{1} &amp; v_{2}^Tv_{2} &amp; \cdots &amp; v_{2}^Tv_{p} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
  v_{p}^Tv_{1} &amp; v_{p}^Tv_{2} &amp; \cdots &amp; v_{p}^Tv_{p}
\end{pmatrix}.
\tag {7}\]

<p>Thus,</p>

\[w_{jj'} = v_j^Tv_{j'} = \langle v_j, v_{j'} \rangle = (v_{1j}, v_{2j}, \cdots, v_{kj}) 
\begin{pmatrix}
v_{1j'} \\ v_{2j'} \\ \cdots \\ v_{kj'}
\end{pmatrix} 
= \sum_{f=1}^{k} v_{fj}v_{fj'}.
\tag {8}\]

<p>The equation \((2)\) can be rewrite as</p>

\[\hat{y} = w_0 + \sum_{j=1}^p w_jx_j + \sum_{j=1}^p\sum_{j'=j+1}^p \langle v_j, v_{j'} \rangle x_jx_{j'}. \tag {9}\]

<h2 id="learning-factorization-machines">Learning Factorization Machines</h2>

<p>The pairwise interactions can be reformulated:</p>

\[\begin{align}
&amp; \sum_{j=1}^p \sum_{j'=j+1}^p \langle \textbf{v}_j , \textbf{v}_{j'} \rangle x_{j} x_{j'} \\
&amp;= \frac{1}{2} \sum_{j=1}^p \sum_{j'=1}^p \langle \textbf{v}_j , \textbf{v}_{j'} \rangle x_{j} x_{j'} - \frac{1}{2} \sum_{j=1}^p \langle \textbf{v}_j , \textbf{v}_{j} \rangle x_{j} x_{j} \\
&amp;= \frac{1}{2}\left(\sum_{j=1}^p \sum_{j'=1}^p \sum_{f=1}^k v_{fj} v_{fj'} x_{j} x_{j'} \right)  - \frac{1}{2}\left( \sum_{j=1}^p \sum_{f=1}^k v_{fj} v_{fj} x_{j} x_{j} \right) \\
&amp;= \frac{1}{2}\left(\sum_{j=1}^p \sum_{j'=1}^p \sum_{f=1}^k v_{fj} v_{fj'} x_{j} x_{j'}  -  \sum_{j=1}^p \sum_{f=1}^k v_{fj} v_{fj} x_{j} x_{j} \right) \\
&amp;= \frac{1}{2} \sum_{f=1}^{k} \left( \left(\sum_{j=1}^p v_{fj}x_{j} \right) \left( \sum_{j'=1}^p v_{fj'}x_{j'} \right) - \sum_{j=1}^{p} v_{fj}^2 x_{j}^2 \right) \\
&amp;= \frac{1}{2} \sum_{f=1}^{k} \left( \left( \sum_{j=1}^{p} v_{fj}x_{j} \right)^2  - \sum_{j=1}^{p} v_{fj}^2 x_{j}^2 \right). \tag {10}
\end{align}\]

<p>This equation has only linear complexity in both \(k\) and \(p\), i.e. its computation is in \(O(kp)\).</p>

<p>As we have shown, FMs have a closed model equation that can be computed in linear time. The model parameters (\(w_0\), \(w_j\)’s and \(V\)) of FMs can be learned efficiently by gradient descent methods, e.g. stochastic gradient descent (SGD), for a variety of losses, among them are square, logit or hinge loss. The gradient of the FM model is:
\(\begin{align}
\frac{\partial\hat{y}}{\partial\theta} =
\begin{cases}
1,  &amp; \text{if $\theta$ is $w_0$}; \\
x_j, &amp; \text{if $\theta$ is $w_j$}; \\
x_j\sum_{j'=1}^{p} v_{fj'}x_{j'} - v_{fj}x_{j}^2, &amp; \text{if $\theta$ is $v_{fj}$}.
\end{cases}
\end{align} \tag{11}\)</p>

<p>The sum \(\sum_{j'=1}^{p} v_{fj'}x_{j'}\) is independent of \(j\) and thus can be precomputed (e.g. when computing \(\hat{y}\)). In general, each gradient can be computed in constant time \(O(1)\). And all parameter updates for a case \((x,y)\) can be done in \(O(kp)\).</p>

<h2 id="tips-and-summary">Tips and Summary</h2>

<p><strong>Expressiveness</strong>: A FM can express any interaction matrix \(W\) if \(k\) is chosen large enough. Nevertheless in sparse settings, typically a small \(k\) should be chosen because there is not enough data. Restricting \(k\), and also the restricting the expressiveness of the FM, leads to better generalization and thus improved interaction matrix \(W\) under sparsity.</p>

<p><strong>Parameter Estimation Under Sparsity</strong>: In sparse settings, there is usually not enough data to estimate interactions
between variables directly and independently. Factorization machines can estimate interactions even in these settings well because they break the independence of the interaction parameters by factorizing them. In general this means that the data for one interaction helps also to estimate the parameters for related interactions. Read the <a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf">original paper</a> for an intuitive example.</p>

<p><strong>Computation</strong>: The model equation of a factorization machine (equation \((2)\)) can be computed in linear time \(O(kp)\).</p>

<p>In summary, FMs model all possible interactions between values in the feature vector x using factorized interactions instead of full parametrized ones. This has two main advantages:</p>

<ul>
  <li>The interactions between values can be estimated even under high sparsity. Especially, it is possible to generalize to unobserved interactions.</li>
  <li>The number of parameters as well as the time for prediction and learning is linear. This makes direct optimization using SGD feasible and allows optimizing against a variety of loss functions.</li>
</ul>

<p><br /></p>

<p><strong>References:</strong></p>

<p>Rendle, Steffen. “Factorization machines.” <em>2010 IEEE International Conference on Data Mining</em>. IEEE, 2010.</p>

<p>Kafunah, Jefkine. “Factorization machines.” 27 Mar. 2017. https://www.jefkine.com/recsys/2017/03/27/factorization-machines/. Accessed 1 Dec. 2020.</p>


  </div>

  <br> </br>
  <p><font color="grey" size="4"> Comments </font></p>
  <HR color=#D1D0CE SIZE=10>

<div id="disqus_thread"></div>

<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://walkermao.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


                            

  
</div>

      </section>
    </main><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178951885-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178951885-1');
</script>

    
    <div id="back-top">
      <a href="javascript:void(0);" onclick="topFunction()" title="Back to top"> </a>
    </div>

  </body>
</html>

<script src = "/assets/js/scroll_into_view.js"></script>
<script src = "/assets/js/back_to_top.js"></script>