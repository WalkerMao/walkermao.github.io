<!DOCTYPE html>
<html lang="en">
  <head>
    <script src="https://kit.fontawesome.com/123ecac47c.js" crossorigin="anonymous"></script><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="shortcut icon" type="image/png" href="/assets/portfolio.png">
<title>Decision Trees</title>
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Decision Trees | Weikai’s blog.</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Decision Trees" />
<meta name="author" content="Weikai Mao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one. They are conceptually simple yet powerful." />
<meta property="og:description" content="Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one. They are conceptually simple yet powerful." />
<link rel="canonical" href="http://localhost:4000/decision-trees.html" />
<meta property="og:url" content="http://localhost:4000/decision-trees.html" />
<meta property="og:site_name" content="Weikai’s blog." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-11T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Decision Trees" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Weikai Mao"},"dateModified":"2020-03-11T00:00:00+08:00","datePublished":"2020-03-11T00:00:00+08:00","description":"Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one. They are conceptually simple yet powerful.","headline":"Decision Trees","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/decision-trees.html"},"url":"http://localhost:4000/decision-trees.html"}</script>
<!-- End Jekyll SEO tag -->


<meta name="google-site-verification" content="wXp8C1QlYKCpKxfXyFfQXEv9l5fJvcOi53ofYmOcaSA" />
<meta name="msvalidate.01" content="97F0BB32D312B808156DE357EA8474D3" />
<meta name="yandex-verification" content="690106a82d8966ab" />

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ["$","$"], ["\\(","\\)"] ],
        displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno"
      }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>    


  







</head>
  <body>
    <main class="container">
      <section class="about">
        <a href="/" class="iconlink"> 
          <h3> 
            <i class="fas fa-home"></i> HOME 
          </h3> 
        </a>
        <a href="/"> <img src="/assets/portfolio.png" alt="Weikai Mao"></a>
        <h2 id="title"> Weikai Mao </h2>
        <p style="font-size:90%" class="tagline">maoweikai123@outlook.com</p>
        
        <ul class="social"><a href="https://github.com/WalkerMao">
              <li>
                <i class="fab fa-github"></i>
              </li>
            </a><a href="https://www.linkedin.com/in/weikai-mao-000249124">
              <li>
                <i class="fab fa-linkedin"></i>
              </li>
            </a><a href="/wechat-qr-code.html">
              <li>
                <i class="fab fa-weixin"></i>
              </li>
            </a></ul><h3 style="color:gray; font-weight:normal"> Categories: </h3>
            <ul class="post-categories" style="max-width:225px;">
              
                  <a class="post-link" href="/"> <li style="padding: 4px 8px;"> All </li> </a>
              
                  <a class="post-link" href="/categories/cs"> <li style="padding: 4px 8px;"> CS </li> </a>
              
                  <a class="post-link" href="/categories/cv"> <li style="padding: 4px 8px;"> CV </li> </a>
              
                  <a class="post-link" href="/categories/dl"> <li style="padding: 4px 8px;"> DL </li> </a>
              
                  <a class="post-link" href="/categories/fe"> <li style="padding: 4px 8px;"> FE </li> </a>
              
                  <a class="post-link" href="/categories/math"> <li style="padding: 4px 8px;"> Math </li> </a>
              
                  <a class="post-link" href="/categories/ml"> <li style="padding: 4px 8px;"> ML </li> </a>
              
                  <a class="post-link" href="/categories/nlp"> <li style="padding: 4px 8px;"> NLP </li> </a>
              
                  <a class="post-link" href="/categories/stat"> <li style="padding: 4px 8px;"> Stat </li> </a>
              
                  <a class="post-link" href="/categories/杂"> <li style="padding: 4px 8px;"> 杂 </li> </a>
              
            </ul><p>&copy; 2023 </p>

      </section>
      <section class="content">
        <div class="sidebar">
  <ul><li><a href="#1-regression-trees">1. Regression Trees</a><ul><li><a href="#11-how-to-build-a-regression-tree">1.1 How to build a regression tree?</a></li><li><a href="#12-how-to-decide-the-tree-size">1.2 How to decide the tree size?</a></li></ul></li><li><a href="#2-classification-trees">2. Classification Trees</a></li><li><a href="#3-interpretation">3. Interpretation</a><ul><li><a href="#3-1-relative-importance-of-predictor-variables">3. 1 Relative Importance of Predictor Variables</a></li><li><a href="#32-partial-dependence-plots">3.2 Partial Dependence Plots</a></li></ul></li><li><a href="#4-some-tips">4. Some Tips</a></li></ul>

</div><div class="post-container" id="viewpoint">
  <a class="post-link" href="/decision-trees.html">
    <h2 class="post-title">Decision Trees</h2>
  </a>
  <div class="post-meta">
    <div>
      <ul class="post-categories"><a class="post-link" href="/categories/ml"> <li>ML</li> </a></ul>
      <ul class="post-tags"><a class="post-link" href="/tags/ml models"> <li>ML models</li> </a></ul>
    </div>
    <div class="post-date">
      <i class="icon-calendar"></i>
      Mar 11, 2020
    </div>
  </div>
  <div style="line-height:77%;">
    <br>
  </div>
  <div class="post"><p>Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one. They are conceptually simple yet powerful.</p>

<div style="text-align: center">
<img src="../../pictures/Partitions-of-tree.jpg" alt="Partitions-of-tree" style="zoom:29%;" />    <img src="../../pictures/Decision-tree.jpg" alt="Decision-tree" style="zoom:30%;" /> 
</div>

<h2 id="1-regression-trees">1. Regression Trees</h2>

<p>Suppose first that we have a partition into \(M\) regions $R_1, R_2 , …, R_M$ , and we model the response as a constant $c_m$ in each region:</p>

\[f(x) = \sum_{m=1}^{M} c_m \mathbf{1}(x \in R_m ).\]

<p>If we adopt as our criterion minimization of the sum of squares $(y_i −f (x_i )) ^2$, it is easy to see that the best $\hat{c}_m$ is just the average of $y_i$ in region $R_m$: $\hat{c}_m = \text{avg}(y_i \mid x_i ∈ R_m )$.</p>

<h3 id="11-how-to-build-a-regression-tree">1.1 How to build a regression tree?</h3>

<p>Now finding the best binary partition in terms of minimum <strong>sum of squares</strong> is generally computationally infeasible. Hence we proceed with a <strong>greedy algorithm</strong>. Starting with all of the data, consider a splitting variable \(j\) and split point \(s\), and define the regions of left and right node as</p>

\[R_L(j, s) = \{X \mid X_j \leq s\},\ R_R(j, s) = \{X \mid X_j &gt; s\}.\]

<p>The node splitting criteria is that, we seek the splitting variable $j$ and split point $s$ that solve</p>

\[\min_{j,s}\Big[\min_{c_L}\sum_{x_i \in R_L(j,s)} (y_i-c_L)^2 + \min_{c_R}\sum_{x_i \in R_R(j,s)} (y_i-c_R)^2 \Big].\]

<p>For any choice $j$ and $s$, the inner minimization is solved by</p>

\[\hat{c}_L = \text{avg}(y_i \mid x_i \in R_L (j,s)),\ \hat{c}_R = \text{avg}(y_i \mid x_i \in R_R (j, s)).\]

<p>We index nodes by $m$, with node $m$ representing region $R_m$. Letting</p>

\[N_m = \# \{x_i ∈ R_m \}, \\
\hat{c}_m = \frac{1}{N_m} \sum_{x_i\in R_m} y_i, \\
Q_m = \frac{1}{N_m} \sum_{x_i\in R_m} (y_i-\hat{c}_m)^2,\]

<p>where $Q_m$ is the node impurity, and is measured by squared loss.</p>

<p>Denote \(m_L\) and \(m_R\) as the two child nodes created by splitting node \(m\). Then the <strong>node splitting criteria</strong> can be written as</p>

\[\operatorname*{argmin}_{j,s}\left(N_{m_L}Q_{m_L} + N_{m_R}Q_{m_R} \right).\]

<h3 id="12-how-to-decide-the-tree-size">1.2 How to decide the tree size?</h3>

<p>We first grow a large tree $T_0$, stopping the splitting process only when some minimum node size (say \(5\)) is reached. Then this large tree is pruned using <strong>cost-complexity pruning</strong>.</p>

<p>Either the number of leaf nodes or the depth can be used to measure the complexity of a tree. Here we take the previous one for example.</p>

<p>We define a subtree $T ⊂ T_0$ to be any tree that can be obtained by pruning $T_0$, that is, collapsing any number of its internal (non-terminal) nodes. Let $\mid T \mid$ denote the number of terminal nodes in $T$, which refers to the complexity of the tree.</p>

<p>We define the cost complexity criterion for a tree \(T\) as</p>

\[C_\alpha(T) = \sum_{m=1}^{|T|}N_m Q_m + α|T| = \sum_{m=1}^{|T|}\sum_{x_i\in R_m} (y_i-\hat{c}_m)^2 + α|T|,\]

<p>where \(m\) is the index of terminal node, and \(\alpha\) is the regularization parameter (or strength).</p>

<p>The tuning parameter $α ≥ 0$ governs the trade-off between tree size and its goodness of fit to the data. The idea is to find, for each $α$, the subtree $T_α ⊆ T_0$ to minimize $C_α (T )$.</p>

<p>Large values of $α$ result in smaller trees $T_α$, and conversely for smaller values of \(α\). As the notation suggests, with $α$ = 0 the solution is the full tree $T_0$.</p>

<p>For each \(\alpha\) one can show that there is a unique smallest subtree $T_α$ that minimizes $C_α(T)$. For each $\alpha$, to find $T_α$, we use <strong>weakest link pruning</strong>: we successively collapse the internal node that produces the smallest per-node increase in $\sum_mN_mQ_m$, and continue until we produce the single-node (root) tree. This gives a (finite) sequence of subtrees, and one can show this sequence must contain $T_α$.</p>

<p>To choose $\alpha$, we use <strong>cross-validation</strong>: we choose the value $\hat{\alpha}$ to minimize the cross-validated loss. Our final tree is $T_\hat{\alpha}$.</p>

<h2 id="2-classification-trees">2. Classification Trees</h2>

<p>The only difference between regression trees and classification trees are the methods of splitting nodes and pruning the tree. In a node $m$, representing a region $R_m$ with $N_m$ observations. The proportion of class $k$ observations in node $m$:</p>

\[\hat{p}_{mk} = \frac{1}{N_m} \sum_{x_i \in R_m} I(y_i=k)\]

<p>We classify the observations in node $m$ to the majority class in node $m$:</p>

\[k(m) = \operatorname*{argmax}_k \hat{p}_{mk}.\]

<p>Different measures $Q_m$ of the impurity of node \(m\) include the following:</p>

\[\text{Misclassification error: } 1-\hat{p}_{mk(m)} = \frac{1}{N_m} \sum_{x_i \in R_m} I(y_i \neq k(m)). \\\text{Gini index: } \sum_{k=1}^{K} \hat{p}_{mk} (1-\hat{p}_{mk}). \\\text{Entropy or deviance: } −\sum_{k=1}^{K} \hat{p}_{mk} \log(\hat{p}_{mk}).\]

<p>For binary classification:</p>

\[\text{Misclassification error: } \frac{1}{2} -  \mid \hat{p}_{m1}-\frac{1}{2} \mid. \\\text{Gini index: } 2 \hat{p}_{m1} (1-\hat{p}_{m1}). \\\text{Entropy or deviance: } - \hat{p}_{m1} \log(\hat{p}_{m1}) - (1-\hat{p}_{m1}) \log(1-\hat{p}_{m1}).\]

<div style="text-align: center"> <img src="/pictures/Three-measures.png" alt="Three-measures" style="zoom: 70%;" /> </div>

<p>All three are similar, but entropy and the Gini index are differentiable, and hence more amenable to numerical optimization.</p>

<p>Similar to regression tree, the node splitting criteria is that, finding the splitting variable $j$ and split point $s$ by</p>

\[\operatorname*{argmin}_{j,s}\left(N_{m_L}Q_{m_L} + N_{m_R}Q_{m_R} \right).\]

<p>We can also define a criteria called <strong>gain</strong> to measure the reduction in impurity after splitting, such as  <strong>information gain</strong> that measures the reduction in entropy.</p>

<p>We select the feature and splitting point for a node that maximize the gain. After splitting the node $m$ to two child nodes $m_L$ (left node) and $m_R$ (right node), we can calculate the gain as</p>

\[\text{Gain}_m = Q_m - \frac{N_{m_L}}{N_m} Q_{m_L} - \frac{N_{m_R}}{N_m} Q_{m_R}.\]

<p>Then the node splitting criteria can be written as</p>

\[\operatorname*{argmin}_{j,s} \text{Gain}_m.\]

<p>Gini index and entropy are more sensitive to changes in the node probabilities than the misclassification rate. For example, in a two-class problem with 400 observations in each class (denote this by (400, 400)), suppose one split created nodes (300, 100) and (100, 300), while the other created nodes (200, 400) and (200, 0). Both splits produce a misclassification rate of 0.25, but the second split produces a pure node and is probably preferable. Both the Gini index and entropy are lower for the second split.</p>

<p>For this reason, either the Gini index or entropy should be used when growing the tree. To guide cost-complexity pruning, any of the three measures can be used, but typically it is the misclassification rate.</p>

<p>Growing a tree: Gini index, entropy. Cost-complexity pruning: any of the three.</p>

<p>We usually treat the class proportions in the terminal node as the class probability estimates.</p>

<h2 id="3-interpretation">3. Interpretation</h2>

<p>Single decision trees can be graphed and are highly interpretable, but the ensemble trees must be interpreted in the different ways as follow. The following methods can be used for both single tree and ensemble trees.</p>

<h3 id="3-1-relative-importance-of-predictor-variables">3. 1 Relative Importance of Predictor Variables</h3>

<p>For a single decision tree $T_m$, the importance of the variable \(X_j\) (denote as \(\mathcal{I}_j^2(T_m)\)) can be measured by the sum of the improvements in loss after partition regions by variable $X_j$ over all nodes.</p>

<p>For additive tree expansions, it is simply averaged over the trees \(\mathcal{I}_j^2 = \frac{1}{M} \sum_{m=1}^{M} \mathcal{I}_j^2(T_m)\).</p>

<h3 id="32-partial-dependence-plots">3.2 Partial Dependence Plots</h3>

<p>Let $\mathcal{S} ⊂ {1,2,…,p}$. Let $\mathcal{C}$ be the complement set of  $\mathcal{S}$, with $\mathcal{S} ∪ \mathcal{C} = {1,2,…,p}$. The goal is to produce a visual description of the effect of $X_S$ on $f$ via a plot of $\hat{f}(X_S)$. A general function $f(X)$ will in principle depend on all of the input variables: $f(X) = f(X_\mathcal{S},X_\mathcal{C})$.</p>

<p>The average or partial dependence of $f(X)$ on $X_\mathcal{S}$ can be defined as \(f_\mathcal{S}(X_\mathcal{S}) = E_{X_\mathcal{C}} f (X_\mathcal{S}, X_\mathcal{C})\), and can be estimated by</p>

\[\bar{f}_\mathcal{S}(X_\mathcal{S}) = \frac{1}{N} \sum_{i=1}^{N} f(X_\mathcal{S}, x_\mathcal{iC}),\]

<p>where ${x_{1\mathcal{C}},…,x_{N\mathcal{C}} }$ are the values of $X_\mathcal{C}$ occurring in the training data.</p>

<p>Partial dependence functions \(f_\mathcal{S}(X_\mathcal{S})\) can be used to interpret the results of <strong>any</strong> “black box” learning method. However, it can be computationally intensive. Fortunately with decision trees, \(\bar{f}_\mathcal{S}(X_\mathcal{S})\) can be rapidly computed from the tree itself without reference to the data (ESL Exercise 10.11).</p>

<p>The partial dependence of $f(X)$ on $X_\mathcal{S}$ is a marginal average of $f$, and can represent the <strong>effect</strong> of $X_\mathcal{S}$ on $f(X)$ after accounting for the (average) effects of the other variables $X_\mathcal{C}$ on $f(X)$. It is not the effect of \(X_\mathcal{S}\) on $f(X)$ ignoring the effects of $X_\mathcal{C}$. The latter is given by the conditional expectation \(\tilde{f}_\mathcal{S}(X_\mathcal{S}) = E[f(X_\mathcal{S}, X_\mathcal{C}) \mid f(X_\mathcal{S})]\).</p>

<div style="text-align: center">
<img src="../../../pictures/Partial-dependence-plot-1.jpg" alt="Partial-dependence-plot-1" style="zoom: 33%;" /><img src="../../../pictures/Partial-dependence-plot-2.jpg" alt="Partial-dependence-plot-2" style="zoom: 33%;" /><img src="../../../pictures/Partial-dependence-plot-3.jpg" alt="Partial-dependence-plot-3" style="zoom: 28.5%;" />
</div>

<p>The plots above show the partial dependence of house value on average occupancy and house age. There appears to be a strong interaction effect between these two variables.</p>

<h2 id="4-some-tips">4. Some Tips</h2>

<p>Time complexity analysis: Suppose we have \(N\) observations with \(p\) features. Splitting a non-leaf node with \(N_m\) observations costs \(N_mp\) time, and the total number of observations in each level of the tree is \(N\), and it follows that the training time costs in each level are all \(Np\). Thus the time complexity of training a tree of depth \(d\) is \(O(Npd)\). If every leaf node contains one observation, then \(d=\log_2N\) in the best case of a balanced tree and \(d=N-1\) in the worst case of a skewed tree. Also obviously, the prediction time complexity for a input is \(O(d)\).</p>

<p>A key advantage of the recursive binary tree is its interpretability. However, one major problem with trees is their high variance, which makes interpretation somewhat precarious. The major reason for this instability is the hierarchical nature of the process: the effect of an error in the top split is propagated down to all of the splits below it. Bagging averages many trees to reduce the variance.</p>

<p>Tree or ensemble tree methods are not suitable for high dimensional sparse data. (<a href="https://zhuanlan.zhihu.com/p/88234363">Explanation</a> in Chinese)</p>

<p>Some aspects of decision tree learning:</p>

<ul>
  <li>Time complexity (\(N\): number of observations; \(p\): number of features; \(d\): depth of the tree):
    <ul>
      <li>Training: \(O(Npd)\);</li>
      <li>Prediction: \(O(d)\).</li>
    </ul>
  </li>
  <li>
    <p>Multi-way splits?</p>

    <ul>
      <li>fragments the data very quickly;</li>
      <li>you can always realize a multi-way split by doing a series of binary splits.</li>
    </ul>
  </li>
  <li>
    <p>Linear combinations of splits? E.x. $R_1 = {X: 2X_1+X_2 &lt; 0 }, R_2 = {X: 2X_1+X_2 \geq 0 }$ .</p>

    <ul>
      <li>maybe good for prediction;</li>
      <li>not good for interpretation;</li>
      <li>expensive for computation.</li>
    </ul>
  </li>
  <li>
    <p>What types of functions does regression tree have trouble approximating?</p>

    <ul>
      <li>additive function $f(X) = \sum_{j=1}^{p} f_j(X_j)$. e.x. $f(X)=\sum_{j=1}^{p} \beta_j X_j$. (note that $Y=f(X)+\epsilon$)</li>
      <li>$f(X) = I{X_1 \leq t_1 } + I{X_2 \leq t_2 }$.</li>
    </ul>
  </li>
  <li>
    <p>Ways to handle missing data</p>

    <ul>
      <li>
        <p>remove observations containing missing values;</p>
      </li>
      <li>
        <p>add category “missing” and check whether “missing” has any predictive value;</p>
      </li>
      <li>
        <p>surrogate splits.</p>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<p><strong>Reference:</strong></p>

<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. <em>The elements of statistical learning</em>. Vol. 1. No. 10. New York: Springer series in statistics, 2001.</p>

  </div>

  <br> </br>
  <p><font color="grey" size="4"> Comments </font></p>
  <HR color=#D1D0CE SIZE=10>

<div id="disqus_thread"></div>

<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://walkermao.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


                            

  
</div>

      </section>
    </main><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178951885-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178951885-1');
</script>

    
    <div id="back-top">
      <a href="javascript:void(0);" onclick="topFunction()" title="Back to top"> </a>
    </div>

  </body>
</html>

<script src = "/assets/js/scroll_into_view.js"></script>
<script src = "/assets/js/back_to_top.js"></script>